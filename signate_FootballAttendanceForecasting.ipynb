{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# チームいわな\n",
    "\n",
    "最終的な精度　：RMSE 3251（40位 / 486チーム）  \n",
    "モデル作成手法：勾配ブースティング\n",
    "\n",
    "作成した主な説明変数  \n",
    "・同一対戦カード別過去の観客動員数平均  \n",
    "・試合直前の順位  \n",
    "・試合直前の連勝数  \n",
    "・2014年World Cup出場選手数  \n",
    "・対戦チームの年棒総和  \n",
    "・天気を「晴」「雨」に統一  \n",
    "・曜日を「休日」「平日」に統一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "・苦労した点  \n",
    "①過去の観客動員数平均を計算するときに  \n",
    "　平日や雨の影響で平均値がぶれる。  \n",
    "②セレッソ大阪などは観客動員数が増加傾向。  \n",
    "　ただ、年平均成長率（CAGR）を入れると過学習して精度悪化。  \n",
    "③validationを2013/08以降のデータでやりたいが、学習にも使いたい。  \n",
    "　いったん2013/08以降で精度確かめて、全データで学習しなおすのはやり方として微妙？  \n",
    "　そもそも勾配ブースティングを少ないサンプルにやるのは筋が悪い？　\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling as pdp\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from IPython.core.display import display\n",
    "\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\",  200)\n",
    "pd.set_option(\"display.max_rows\",     2000)\n",
    "pd.set_option(\"display.max_colwidth\", 2000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "region_pref = {\"北海道\":[\"北海道\"],\n",
    "               \"東北\"  :[\"青森県\",\"岩手県\",\"秋田県\",\"宮城県\",\"山形県\",\"福島県\"],\n",
    "               \"関東\"  :[\"茨城県\",\"栃木県\",\"群馬県\",\"埼玉県\",\"千葉県\",\"東京都\",\"神奈川県\"],\n",
    "               \"中部\"  :[\"新潟県\",\"富山県\",\"石川県\",\"福井県\",\"山梨県\",\"長野県\",\"岐阜県\",\"静岡県\",\"愛知県\"],\n",
    "               \"近畿\"  :[\"三重県\",\"滋賀県\",\"奈良県\",\"和歌山県\",\"京都府\",\"大阪府\",\"兵庫県\"],\n",
    "               \"中国\"  :[\"岡山県\",\"広島県\",\"鳥取県\",\"島根県\",\"山口県\"],\n",
    "               \"四国\"  :[\"香川県\",\"徳島県\",\"愛媛県\",\"高知県\"],\n",
    "               \"九州\"  :[\"福岡県\",\"佐賀県\",\"長崎県\",\"大分県\",\"熊本県\",\"宮崎県\",\"鹿児島県\",\"沖縄県\"]}\n",
    "near_region = {\"北海道\" : [\"東海\"],\n",
    "               \"東北\"   : [\"北海道\",\"関東\"],\n",
    "               \"関東\"   : [\"東北\",\"中部\"],\n",
    "               \"中部\"   : [\"関東\",\"近畿\"],\n",
    "               \"近畿\"   : [\"東海\",\"中国\"],\n",
    "               \"中国\"   : [\"近畿\",\"四国\"],\n",
    "               \"四国\"   : [\"中国\",\"九州\"],\n",
    "               \"九州\"   : [\"中国\",\"四国\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# カテゴリデータのダミー変数化\n",
    "def dummies(df, cols):\n",
    "    df_droped  = copy.deepcopy(df.drop(cols, axis=1)).reset_index(drop=True)\n",
    "    df_dummies = df.reset_index(drop=True)\n",
    "    df_dummies = pd.get_dummies(df_dummies[cols], drop_first=True)\n",
    "    return pd.merge(df_droped, df_dummies, left_index=True, right_index=True)\n",
    "\n",
    "# 数値データの標準化\n",
    "def standardization(df, cols, df_test=None):\n",
    "    mean     = df[cols].mean()\n",
    "    std      = df[cols].std()\n",
    "    cols_std = [col + \"_std\" for col in cols]\n",
    "    df_std   = copy.deepcopy(df)\n",
    "    df_std[cols_std] = df_std[cols].apply(lambda x: (x - mean[x.name]) / std[x.name])\n",
    "    df_test_std      = copy.deepcopy(df_test)\n",
    "    if df_test is not None:\n",
    "        df_test_std[cols_std] = df_test_std[cols].apply(lambda x: (x - mean[x.name]) / std[x.name])\n",
    "    return df_std, df_test_std\n",
    "\n",
    "def re_weather(df):\n",
    "    df_weather = copy.deepcopy(df)\n",
    "    df_weather.weather = df_weather.weather.apply(lambda x: x if x.find(\"のち\")==-1 else x.split(\"のち\")[1])\n",
    "    df_weather.weather = df_weather.weather.apply(lambda x: x.split(\"一時\")[0])\n",
    "    df_weather.weather = df_weather.weather.apply(lambda x: x.split(\"時々\")[0])\n",
    "    df_weather.weather = df_weather.weather.apply(lambda x: \"晴\" if x in [\"屋内\",\"曇\"] else x)\n",
    "    df_weather.weather = df_weather.weather.apply(lambda x: \"雨\" if x in [\"雷雨\",\"雪\",\"霧\"] else x.replace(\" \",\"\"))\n",
    "    return df_weather\n",
    "\n",
    "def re_weekday(df):\n",
    "    public_holiday = [\"2011年1月1日\",\"2011年1月10日\",\"2011年2月11日\",\"2011年3月21日\",\"2011年4月29日\",\n",
    "                      \"2011年5月3日\",\"2011年5月4日\",\"2011年5月5日\",\"2011年7月18日\",\"2011年9月19日\",\n",
    "                      \"2011年9月23日\",\"2011年10月10日\",\"2011年11月3日\",\"2011年11月23日\",\"2011年12月23日\"]\n",
    "    df_weekday = copy.deepcopy(df)\n",
    "    df_weekday.weekday = df_weekday.apply(lambda x: \"休\" if -1 < x.gameday.find(\"祝\") else x.weekday, axis=1)\n",
    "    df_weekday.weekday = df_weekday.apply(lambda x: \"休\" if -1 < x.gameday.find(\"休\") else x.weekday, axis=1)\n",
    "    df_weekday.weekday = df_weekday.apply(lambda x: \"休\" if x.gameday in public_holiday else x.weekday, axis=1)\n",
    "    df_weekday.loc[ df_weekday.weekday.isin([\"土\",\"日\"]), \"weekday\"] = \"休\"\n",
    "    df_weekday.loc[~df_weekday.weekday.isin([\"休\"]),      \"weekday\"] = \"平\"\n",
    "    return df_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "path    = \"../../../../study/jleague/motodata/\"\n",
    "dict_df = {}\n",
    "for file in os.listdir(path):\n",
    "    if file.find(\".csv\")==-1 or -1 < file.find(\"sample\"): continue\n",
    "    filename = os.path.splitext(file)[0]\n",
    "    if   filename==\"train_add\":\n",
    "        dict_df[\"train\"]     = pd.concat([dict_df[\"train\"],     pd.read_csv(path + file)]).reset_index(drop=True)\n",
    "        dict_df[\"train\"].loc[dict_df[\"train\"].home==\"ザスパ草津\", \"home\"] = \"ザスパクサツ群馬\"\n",
    "        dict_df[\"train\"].loc[dict_df[\"train\"].away==\"ザスパ草津\", \"away\"] = \"ザスパクサツ群馬\"\n",
    "    elif filename==\"condition_add\":\n",
    "        dict_df[\"condition\"] = pd.concat([dict_df[\"condition\"], pd.read_csv(path + file)]).reset_index(drop=True)\n",
    "    elif filename==\"test\":\n",
    "        dict_df[\"test\"]      = pd.concat([dict_df[\"2014_add\"],  pd.read_csv(path + file)]).reset_index(drop=True)\n",
    "    else:\n",
    "        dict_df[filename]    = pd.read_csv(path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for key, df in dict_df.items():\n",
    "    print(key, df.shape)\n",
    "    display(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#クラブチームと都道府県\n",
    "url  = \"http://zatutisiki.com/7376.html\"\n",
    "soup = BeautifulSoup(requests.get(url).text, 'lxml')\n",
    "list_pref_team = []\n",
    "for row in soup.find_all(\"td\"):\n",
    "    #都道府県名\n",
    "    if row.get(\"class\")[0]==\"column-1\":\n",
    "        pref = row.get_text()\n",
    "    #チーム名\n",
    "    if row.get(\"class\")[0]==\"column-2\":\n",
    "        if pref==\"\": continue\n",
    "        list_pref_team.append([pref, row.get_text()])\n",
    "df_pref_team = pd.DataFrame(list_pref_team, columns=[\"pref\",\"home\"])\n",
    "df_pref_team.loc[df_pref_team.home==\"ベカルタ仙台\",          \"home\"] = \"ベガルタ仙台\"\n",
    "df_pref_team.loc[df_pref_team.home==\"北海道コンサドーレ札幌\",\"home\"] = \"コンサドーレ札幌\"\n",
    "\n",
    "for pref in df_pref_team.pref.unique():\n",
    "    for region, prefs in region_pref.items():\n",
    "        if pref in prefs: df_pref_team.loc[df_pref_team.pref==pref, \"region\"] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#チーム別の所在都道府県、スタジアム情報\n",
    "df_stadium = dict_df['stadium']\n",
    "df_stadium.rename(columns={\"name\":\"stadium\"}, inplace=True)\n",
    "df_stadium.capa    = df_stadium.capa.astype(int)\n",
    "df_stadium[\"pref\"] = df_stadium.address.apply(lambda x: x[0:3])\n",
    "df_stadium.loc[df_stadium.pref==\"神奈川\", \"pref\"] += \"県\"\n",
    "\n",
    "df_team_info = pd.concat([dict_df[\"train\"], dict_df[\"test\"]])\n",
    "df_team_info = df_team_info[[\"home\",\"stadium\"]].drop_duplicates()\n",
    "df_team_info.groupby(\"home\", as_index=False).stadium.count()\n",
    "df_team_info[\"home_nor\"] = df_team_info.home.apply(lambda x:unicodedata.normalize(\"NFKC\", x))\n",
    "df_team_info = pd.merge(df_team_info, df_pref_team, left_on=\"home_nor\", right_on=\"home\", how=\"outer\", suffixes=(\"\",\"_tmp\"))\n",
    "df_team_info.drop([\"home_tmp\",\"home_nor\"], axis=1, inplace=True)\n",
    "df_team_info.loc[df_team_info.home==\"Ｖ・ファーレン長崎\", \"region\"] = \"九州\"\n",
    "df_team_info.loc[df_team_info.home==\"Ｖ・ファーレン長崎\", \"pref\"]   = \"長崎県\"\n",
    "df_team_info = df_team_info.dropna()\n",
    "df_team_info = pd.merge(df_team_info, df_stadium, on=\"stadium\", suffixes=(\"_team\",\"_stadium\"))\n",
    "df_team_info[\"local_stadium_opened\"] = df_team_info.apply(lambda x: 1 if x[\"pref_team\"]==x[\"pref_stadium\"] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#同一地区、隣接地区\n",
    "df_region = pd.concat([dict_df[\"train\"], dict_df[\"test\"]])\n",
    "df_region = pd.merge(df_region, df_team_info, on=[\"home\",\"stadium\"])\n",
    "df_region = pd.merge(df_region, df_team_info[[\"home\",\"region\"]].drop_duplicates(), left_on=\"away\", right_on=\"home\", suffixes=(\"\",\"_away\"))\n",
    "df_region = df_region.assign(same_region = df_region.apply(lambda x: 1 if x.region==x.region_away else 0, axis=1),\n",
    "                             near_region = df_region.apply(lambda x: 1 if x.region_away in near_region[x.region] else 0, axis=1))\n",
    "df_region = df_region[[\"id\",\"same_region\",\"near_region\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#2011年の観客動員数\n",
    "def get_past_data(year):\n",
    "    statime  = datetime.now()\n",
    "    df_past  = pd.DataFrame()\n",
    "    df_teams = pd.concat([dict_df['train'], dict_df['test']])\n",
    "    df_teams[\"team_nor\"] = df_teams.home.apply(lambda x: unicodedata.normalize(\"NFKC\", x))\n",
    "    df_teams = df_teams[[\"home\",\"team_nor\"]].drop_duplicates()\n",
    "    for team, team_nor in zip(df_teams.home, df_teams.team_nor):\n",
    "        search   = team_nor\n",
    "        if team_nor==\"東京ヴェルディ\" : search = \"東京ヴェルディ1969\"\n",
    "        if team_nor==\"FC町田ゼルビア\" : search = \"町田ゼルビア\"\n",
    "        url      = \"http://footballgeist.com/audience?id=team&no=\" + search + \"&season=\" + str(year) + \"#setubetu\"\n",
    "        soup     = BeautifulSoup(requests.get(url).text, 'lxml')\n",
    "        row_team = soup.find_all(\"table\")[2]\n",
    "        if row_team.find(\"td\") is None: continue\n",
    "\n",
    "        team_info = []\n",
    "        for row_team_td in row_team.find_all(\"td\"):\n",
    "            if row_team_td.find(\"a\") is not None:\n",
    "                href = row_team_td.find(\"a\").get(\"href\")\n",
    "                if -1 < href.find(\"team\"):\n",
    "                    away_team = href.replace(\"team/\",\"\")\n",
    "                    df_away   = df_teams.query(\"team_nor==@away_team\")\n",
    "                    if df_away.shape[0] != 0           : away_team = df_away.home.values[0]\n",
    "                    if away_team==\"東京ヴェルディ1969\" : away_team = \"東京ヴェルディ\"\n",
    "                    team_info.append(away_team)\n",
    "                if -1 < href.find(\"match\"):\n",
    "                    match      = href.replace(\"match/\",\"\")\n",
    "                    url        = \"http://footballgeist.com/match/\" + match\n",
    "                    row_match  = BeautifulSoup(requests.get(url).text, 'lxml').find_all(\"table\")[0]\n",
    "                    row_match  = row_match.find_all(\"td\")\n",
    "                    stage      = np.nan\n",
    "                    if row_match[0].get_text()==\"J1\": stage = \"Ｊ１\"\n",
    "                    if row_match[0].get_text()==\"J2\": stage = \"Ｊ２\"\n",
    "                    team_info += [stage, row_match[1].get_text(), row_match[6].get_text()]\n",
    "            else:\n",
    "                team_info.append(row_team_td.get_text())\n",
    "\n",
    "        df      = pd.DataFrame(np.array(team_info).reshape(-1, 8),\n",
    "                               columns=[\"section\",\"away\",\"stage\",\"gameday\",\"weather\",\"y\",\"ratio\",\"stadium\"]).assign(year=year, home=team)\n",
    "        df_past = pd.concat([df_past, df])\n",
    "\n",
    "    weekday = [\"月\",\"火\",\"水\",\"木\",\"金\",\"土\",\"日\"]\n",
    "    df_past = df_past.assign(y          = df_past.y.astype(int),\n",
    "                             section    = df_past.section.astype(int),\n",
    "                             weekday    = df_past.gameday.apply(lambda x: weekday[pd.to_datetime(x.replace(\"年\",\"/\").replace(\"月\",\"/\").replace(\"日\",\"\")).weekday()]))\n",
    "    df_past[\"y_ratio\"] = df_past.ratio.apply(lambda x: x.replace(\"%\",\"\").replace(\"-\",\"0\")).astype(float) / 100\n",
    "    df_past[\"y_ratio\"] = np.log(df_past.y_ratio / (1 - df_past.y_ratio))\n",
    "    print(datetime.now()-statime)\n",
    "    return df_past\n",
    "\n",
    "df_10 = get_past_data(2010)\n",
    "df_11 = get_past_data(2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#試合直前の順位\n",
    "df_score = copy.deepcopy(dict_df['condition'])\n",
    "df_score[\"home_win_point\"] = df_score.apply(lambda x: 1 if x[\"home_score\"]==x[\"away_score\"] else 3 if x[\"home_score\"] > x[\"away_score\"] else 0, axis=1)\n",
    "df_score[\"away_win_point\"] = df_score.apply(lambda x: 1 if x[\"home_score\"]==x[\"away_score\"] else 0 if x[\"home_score\"] > x[\"away_score\"] else 3, axis=1)\n",
    "df_train_test = pd.concat([dict_df['train'], dict_df['test']])\n",
    "df_score      = pd.merge(df_train_test, df_score, on=\"id\")\n",
    "df_home_score = df_score[[\"id\",\"year\",\"gameday\",\"stage\",\"home\",\"home_score\",\"away_score\",\"home_win_point\"]]\n",
    "df_home_score[\"home_away\"] = 1\n",
    "df_home_score.rename(columns={\"home\":\"team\", \"home_win_point\":\"win_point\", \"home_score\":\"got_score\", \"away_score\":\"lost_score\"}, inplace=True)\n",
    "df_away_score = df_score[[\"id\",\"year\",\"gameday\",\"stage\",\"away\",\"away_score\",\"home_score\",\"away_win_point\"]]\n",
    "df_away_score[\"home_away\"] = 0\n",
    "df_away_score.rename(columns={\"away\":\"team\", \"away_win_point\":\"win_point\", \"away_score\":\"got_score\", \"home_score\":\"lost_score\"}, inplace=True)\n",
    "df_score_cnt  = pd.concat([df_home_score, df_away_score])\n",
    "df_score_cnt[\"diff_score\"] = df_score_cnt.got_score - df_score_cnt.lost_score\n",
    "df_score_cnt  = df_score_cnt.sort_values(by=[\"year\",\"team\",\"id\"], ascending=[True,True,True])\n",
    "\n",
    "win_points, got_scores, diff_scores  = [], [], []\n",
    "for year in df_score_cnt.year.unique():\n",
    "    for team in df_score_cnt.team.unique():\n",
    "        win_points  += list(df_score_cnt.query(\"year==@year & team==@team\").win_point.cumsum())\n",
    "        got_scores  += list(df_score_cnt.query(\"year==@year & team==@team\").got_score.cumsum())\n",
    "        diff_scores += list(df_score_cnt.query(\"year==@year & team==@team\").diff_score.cumsum())\n",
    "df_score_cnt = df_score_cnt.assign(win_point_sum  = win_points,\n",
    "                                   got_score_sum  = got_scores,\n",
    "                                   diff_score_sum = diff_scores)\n",
    "ranked_team = []\n",
    "for stage in df_score_cnt.stage.unique():\n",
    "    for year in df_score_cnt.year.unique():\n",
    "        df_all_team = df_score_cnt.query(\"year==@year & stage==@stage\")[[\"team\"]].drop_duplicates()\n",
    "        for day in df_score_cnt.query(\"year==@year & stage==@stage\").gameday.unique():\n",
    "            df_rank = df_score_cnt.query(\"year==@year & gameday < @day & stage==@stage\")\n",
    "            df_rank = df_rank.groupby([\"year\",\"stage\",\"team\"], as_index=False).gameday.max()\n",
    "            if df_rank.shape[0]==0:\n",
    "                #その年初めての試合\n",
    "                df_rank_wide = df_score_cnt.query(\"year==@year & gameday==@day & stage==@stage\")\n",
    "                df_rank_wide = df_rank_wide[[\"id\"]].drop_duplicates().assign(ranking_home=1, ranking_away=1)\n",
    "            else:\n",
    "                #その年２回目以降の試合\n",
    "                df_rank = pd.merge(df_all_team,  df_rank, on=\"team\", how=\"left\")\n",
    "                df_rank = pd.merge(df_rank, df_score_cnt, on=[\"team\",\"year\",\"stage\",\"gameday\"], how=\"left\")\n",
    "                df_rank.fillna(0, inplace=True)\n",
    "                df_rank.loc[df_rank.stage==0, \"home_away\"] = 2\n",
    "                df_rank = df_rank.sort_values(by=[\"win_point_sum\",\"got_score_sum\",\"diff_score_sum\"], ascending=[False,False,False]).reset_index(drop=False)\n",
    "                df_rank = df_rank.assign(diff_other_team1 = df_rank.win_point_sum.diff(),\n",
    "                                         diff_other_team2 = df_rank.got_score_sum.diff(),\n",
    "                                         diff_other_team3 = df_rank.diff_score_sum.diff(),\n",
    "                                         ranking          = [i+1 for i in range(df_rank.shape[0])])\n",
    "                #勝ち点、総得点、得失点差が同じ場合はランキングを同じにする\n",
    "                for _, row in df_rank.query(\"diff_other_team1==0 & diff_other_team2==0 & diff_other_team3==0\").iterrows():\n",
    "                    df_rank.loc[row.name, \"ranking\"] = df_rank.loc[row.name-1, \"ranking\"]\n",
    "\n",
    "                df_rank_wide = pd.merge(df_score_cnt.query(\"gameday==@day\"), df_rank[[\"team\",\"ranking\"]], on=\"team\")\n",
    "                df_rank_wide = pd.merge(df_rank_wide.query(\"home_away==1\"),\n",
    "                                        df_rank_wide.query(\"home_away==0\"), on=[\"id\",\"year\",\"gameday\",\"stage\"], suffixes=(\"_home\",\"_away\"))\n",
    "                df_rank_wide = df_rank_wide[[\"id\",\"ranking_home\",\"ranking_away\"]]    \n",
    "            ranked_team.append(df_rank_wide)\n",
    "            \n",
    "df_ranked_team = pd.concat(ranked_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#対戦カード別過去の観客動員数\n",
    "df_12_14  = pd.concat([dict_df['train'],dict_df['test']])\n",
    "e_cols    = [\"home\",\"away\",\"y\"]\n",
    "m_cols    = [\"home\",\"away\"]\n",
    "df_y_mean = pd.merge(df_10[e_cols], df_11[e_cols],                        on=m_cols, how=\"outer\", suffixes=(\"_10\",\"_11\"))\n",
    "df_y_mean = pd.merge(df_y_mean,     df_12_14.query(\"year==2012\")[e_cols], on=m_cols, how=\"outer\", suffixes=(\"_11\",\"_12\"))\n",
    "df_y_mean = pd.merge(df_y_mean,     df_12_14.query(\"year==2013\")[e_cols], on=m_cols, how=\"outer\", suffixes=(\"_12\",\"_13\"))\n",
    "df_y_mean = pd.merge(df_y_mean,     df_12_14.query(\"year==2014\")[e_cols], on=m_cols, how=\"outer\", suffixes=(\"_13\",\"_14\"))\n",
    "df_y_mean.rename(columns={\"y\":\"y_14\"}, inplace=True)\n",
    "e_cols    = [\"home\",\"stage\"]\n",
    "df_stage  = pd.merge(df_10[e_cols].drop_duplicates(), df_11[e_cols].drop_duplicates(), on=\"home\", how=\"outer\", suffixes=(\"_10\",\"_11\"))\n",
    "df_stage  = pd.merge(df_stage, df_12_14.query(\"year==2012\")[e_cols].drop_duplicates(), on=\"home\", how=\"outer\", suffixes=(\"_11\",\"_12\"))\n",
    "df_stage  = pd.merge(df_stage, df_12_14.query(\"year==2013\")[e_cols].drop_duplicates(), on=\"home\", how=\"outer\", suffixes=(\"_12\",\"_13\"))\n",
    "df_stage  = pd.merge(df_stage, df_12_14.query(\"year==2014\")[e_cols].drop_duplicates(), on=\"home\", how=\"outer\", suffixes=(\"_13\",\"_14\"))\n",
    "df_stage.rename(columns={\"stage\":\"stage_14\"}, inplace=True)\n",
    "df_stage  = df_stage[df_stage.stage_14.notnull()].sort_values(\"home\")\n",
    "\n",
    "df_y_mean_long = pd.DataFrame()\n",
    "for team, stage in zip(df_stage.home, df_stage.stage_14):\n",
    "    df_mean_team  = copy.deepcopy(df_y_mean.query(\"home==@team\"))\n",
    "    df_stage_team = copy.deepcopy(df_stage.query(\"home==@team\"))\n",
    "    #最初のステージ\n",
    "    if   0 < df_stage_team.query(\"stage_10==@stage\").shape[0]:  year = 2010\n",
    "    elif 0 < df_stage_team.query(\"stage_11==@stage\").shape[0]:  year = 2011\n",
    "    elif 0 < df_stage_team.query(\"stage_12==@stage\").shape[0]:  year = 2012\n",
    "    elif 0 < df_stage_team.query(\"stage_13==@stage\").shape[0]:  year = 2013\n",
    "    else:                                                       year = 2014\n",
    "\n",
    "    mean_type=\"gmean\"\n",
    "    def mean_y(df_long, df_mean, year, cols, fill_col):\n",
    "        df = copy.deepcopy(df_mean).assign(year = year)\n",
    "        if   mean_type==\"mean\":   #算術平均\n",
    "            df[\"y_mean\"] = np.nanmean(np.array(df[cols]), axis=1)\n",
    "        elif mean_type==\"gmean\":  #幾何平均\n",
    "            df[\"y_mean\"] = df.fillna(1).apply(lambda x: len(cols)-list(x[cols]).count(1), axis=1)\n",
    "            df[\"y_mean\"] = df.fillna(1).apply(lambda x: math.pow(reduce(operator.mul, list(x[cols])), 1 / x.y_mean if 0 < x.y_mean else 0), axis=1)\n",
    "        df.y_mean.fillna(df[fill_col].mean(), inplace=True)\n",
    "        return pd.concat([df_long, df[[\"home\",\"away\",\"year\",\"y_mean\"]]])\n",
    "    \n",
    "    if year != 2012 and 0 < df_stage_team.query(\"stage_12==@stage\").shape[0]:\n",
    "        df_y_mean_long = mean_y(df_y_mean_long, df_mean_team, 2012, [\"y_10\",\"y_11\"], \"y_11\")\n",
    "    if year != 2013 and 0 < df_stage_team.query(\"stage_13==@stage\").shape[0]:\n",
    "        df_y_mean_long = mean_y(df_y_mean_long, df_mean_team, 2013, [\"y_10\",\"y_11\",\"y_12\"], \"y_12\")\n",
    "    if year != 2014 and 0 < df_stage_team.query(\"stage_14==@stage\").shape[0]:\n",
    "        df_y_mean_long = mean_y(df_y_mean_long, df_mean_team, 2014, [\"y_10\",\"y_11\",\"y_12\",\"y_13\"], \"y_12\")\n",
    "    if year == 2014:\n",
    "        df_y_mean_long = mean_y(df_y_mean_long, df_mean_team, 2014, [\"y_14\"], \"y_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#対戦カード別CAGR\n",
    "df_12_14  = pd.concat([dict_df['train'],dict_df['test']])\n",
    "e_cols    = [\"home\",\"away\",\"y\"]\n",
    "m_cols    = [\"home\",\"away\"]\n",
    "df_y_mean = pd.merge(df_10[e_cols], df_11[e_cols],                        on=m_cols, how=\"outer\", suffixes=(\"_10\",\"_11\"))\n",
    "df_y_mean = pd.merge(df_y_mean,     df_12_14.query(\"year==2012\")[e_cols], on=m_cols, how=\"outer\", suffixes=(\"_11\",\"_12\"))\n",
    "df_y_mean = pd.merge(df_y_mean,     df_12_14.query(\"year==2013\")[e_cols], on=m_cols, how=\"outer\", suffixes=(\"_12\",\"_13\"))\n",
    "df_y_mean = pd.merge(df_y_mean,     df_12_14.query(\"year==2014\")[e_cols], on=m_cols, how=\"outer\", suffixes=(\"_13\",\"_14\"))\n",
    "df_y_mean.rename(columns={\"y\":\"y_14\"}, inplace=True)\n",
    "e_cols    = [\"home\",\"stage\"]\n",
    "df_stage  = pd.merge(df_10[e_cols].drop_duplicates(), df_11[e_cols].drop_duplicates(), on=\"home\", how=\"outer\", suffixes=(\"_10\",\"_11\"))\n",
    "df_stage  = pd.merge(df_stage, df_12_14.query(\"year==2012\")[e_cols].drop_duplicates(), on=\"home\", how=\"outer\", suffixes=(\"_11\",\"_12\"))\n",
    "df_stage  = pd.merge(df_stage, df_12_14.query(\"year==2013\")[e_cols].drop_duplicates(), on=\"home\", how=\"outer\", suffixes=(\"_12\",\"_13\"))\n",
    "df_stage  = pd.merge(df_stage, df_12_14.query(\"year==2014\")[e_cols].drop_duplicates(), on=\"home\", how=\"outer\", suffixes=(\"_13\",\"_14\"))\n",
    "df_stage.rename(columns={\"stage\":\"stage_14\"}, inplace=True)\n",
    "df_stage  = df_stage[df_stage.stage_14.notnull()].sort_values(\"home\")\n",
    "\n",
    "def cagr(a, b, n):\n",
    "    return ((b / a) ** (1 / n)) - 1\n",
    "def check(x, cols_over, cols_0):\n",
    "    return all(0 < y for y in x[cols_over]) and all(y==0 for y in x[cols_0])\n",
    "\n",
    "df_y_mean.fillna(0, inplace=True)\n",
    "df_y_mean[\"cagr_12\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_11, 2) if check(x, [\"y_10\",\"y_11\"],        []) else 0, axis=1)\n",
    "df_y_mean[\"cagr_13\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_12, 3) if check(x, [\"y_10\",\"y_11\",\"y_12\"], []) else 0, axis=1)\n",
    "df_y_mean[\"cagr_13\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_11, 2) if check(x, [\"y_10\",\"y_11\"],  [\"y_12\"]) else x.cagr_13, axis=1)\n",
    "df_y_mean[\"cagr_13\"] = df_y_mean.apply(lambda x: cagr(x.y_11, x.y_12, 2) if check(x, [\"y_11\",\"y_12\"],  [\"y_10\"]) else x.cagr_13, axis=1)\n",
    "\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_13, 4) if check(x, [\"y_10\",\"y_11\",\"y_12\",\"y_13\"], []) else 0, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_13, 3) if check(x, [\"y_10\",\"y_12\",\"y_13\"],  [\"y_11\"]) else x.cagr_14, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_12, 3) if check(x, [\"y_10\",\"y_11\",\"y_12\"],  [\"y_13\"]) else x.cagr_14, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_13, 3) if check(x, [\"y_10\",\"y_11\",\"y_13\"],  [\"y_12\"]) else x.cagr_14, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_11, x.y_13, 3) if check(x, [\"y_11\",\"y_12\",\"y_13\"],  [\"y_10\"]) else x.cagr_14, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_12, x.y_13, 2) if check(x, [\"y_12\",\"y_13\"],  [\"y_10\",\"y_11\"]) else x.cagr_14, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_11, x.y_13, 2) if check(x, [\"y_11\",\"y_13\"],  [\"y_10\",\"y_12\"]) else x.cagr_14, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_11, x.y_12, 2) if check(x, [\"y_11\",\"y_12\"],  [\"y_10\",\"y_13\"]) else x.cagr_14, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_13, 2) if check(x, [\"y_10\",\"y_13\"],  [\"y_11\",\"y_12\"]) else x.cagr_14, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_12, 2) if check(x, [\"y_10\",\"y_12\"],  [\"y_11\",\"y_13\"]) else x.cagr_14, axis=1)\n",
    "df_y_mean[\"cagr_14\"] = df_y_mean.apply(lambda x: cagr(x.y_10, x.y_11, 2) if check(x, [\"y_10\",\"y_11\"],  [\"y_12\",\"y_13\"]) else x.cagr_14, axis=1)\n",
    "\n",
    "df_y_mean_12   = df_y_mean[[\"home\",\"away\",\"cagr_12\"]].assign(year=2012).rename(columns={\"cagr_12\":\"cagr\"})\n",
    "df_y_mean_13   = df_y_mean[[\"home\",\"away\",\"cagr_13\"]].assign(year=2013).rename(columns={\"cagr_13\":\"cagr\"})\n",
    "df_y_mean_14   = df_y_mean[[\"home\",\"away\",\"cagr_14\"]].assign(year=2014).rename(columns={\"cagr_14\":\"cagr\"})\n",
    "df_y_cagr_long = pd.concat([df_y_mean_12, df_y_mean_13, df_y_mean_14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#試合前連勝記録\n",
    "df = copy.deepcopy(pd.concat([dict_df['train'], dict_df['test']]))\n",
    "df = pd.merge(df, dict_df[\"condition\"], on=\"id\")\n",
    "df = df[[\"id\",\"home\",\"away\",\"stage\",\"year\",\"gameday\",\"home_score\",\"away_score\"]]\n",
    "\n",
    "df_series_win = pd.DataFrame()\n",
    "for year in df.year.unique():\n",
    "    for team in df.query(\"year==@year\").home.unique():\n",
    "        df_win = copy.deepcopy(df.query(\"year==@year & (home==@team | away==@team)\"))\n",
    "        df_win[\"win\"] = 0\n",
    "        df_win.loc[(df_win.home==team)&(df_win.away_score <= df_win.home_score), \"win\"] = 1\n",
    "        df_win.loc[(df_win.away==team)&(df_win.home_score <= df_win.away_score), \"win\"] = 1\n",
    "        point, series_points = -1, []\n",
    "        for _, row in df_win.iterrows():\n",
    "            if row.home==team:\n",
    "                home_away = \"home\"\n",
    "            else:\n",
    "                home_away = \"away\"\n",
    "            if row.win==1:\n",
    "                point += 1\n",
    "                series_points.append([home_away, point])\n",
    "            else:\n",
    "                if -1 < point:\n",
    "                    series_points.append([home_away, point + 1])\n",
    "                else:\n",
    "                    series_points.append([home_away, 0])\n",
    "                point  = -1\n",
    "        df_win = df_win.assign(home_away    = np.array(series_points)[:,0],\n",
    "                               series_point = np.array(series_points)[:,1])\n",
    "        df_series_win = pd.concat([df_series_win, df_win])\n",
    "        \n",
    "df_home = df_series_win.query(\"home_away=='home'\")\n",
    "df_away = df_series_win.query(\"home_away=='away'\")[[\"id\",\"series_point\"]]\n",
    "df_series_win = pd.merge(df_home, df_away, on=\"id\", suffixes=(\"_home\",\"_away\"))[[\"id\",\"series_point_home\",\"series_point_away\"]]\n",
    "df_series_win.series_point_away = df_series_win.series_point_away.astype(int)\n",
    "df_series_win.series_point_home = df_series_win.series_point_home.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "path     = \"../../../../study/jleague/team/\"\n",
    "df_wcup  = pd.concat([pd.read_csv(path + \"wcup_train.csv\"),\n",
    "                      pd.read_csv(path + \"wcup_train_add.csv\"),\n",
    "                      pd.read_csv(path + \"wcup_test.csv\")])\n",
    "df_money = pd.concat([pd.read_csv(path + \"money_train.csv\"),\n",
    "                      pd.read_csv(path + \"money_train_add.csv\"),\n",
    "                      pd.read_csv(path + \"money_test.csv\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "    df_add = copy.deepcopy(df)\n",
    "    df_add = df_add.assign(weekday    = df_add.gameday.apply(lambda x: x[x.find(\"(\")+1:x.find(\"(\")+2]),\n",
    "                           section    = df_add.match.apply(lambda x: x[1:x.find(\"節\")]).astype(int),\n",
    "                           time_split = df_add.time.apply(lambda x: x[:2]).astype(int),\n",
    "                           tv_num     = df_add.tv.apply(lambda x: len(x.split(\"／\"))).astype(int),\n",
    "                           champ      = 0)\n",
    "    #前年優勝、準優勝チーム\n",
    "    df_add.loc[(df_add.year==2012)&(df_add.home.isin([\"柏レイソル\",\"名古屋グランパス\"])),         \"champ\"] = 1\n",
    "    df_add.loc[(df_add.year==2013)&(df_add.home.isin([\"サンフレッチェ広島\",\"ベガルタ仙台\"])),     \"champ\"] = 1\n",
    "    df_add.loc[(df_add.year==2014)&(df_add.home.isin([\"サンフレッチェ広島\",\"横浜Ｆ・マリノス\"])), \"champ\"] = 1\n",
    "    #天気、気温\n",
    "    df_add = pd.merge(df_add, dict_df[\"condition\"][[\"id\",\"weather\",\"temperature\"]], on=\"id\")\n",
    "    #スタジアム収容人数、地区\n",
    "    df_add = pd.merge(df_add, df_team_info,   on=[\"home\",\"stadium\"])\n",
    "    #同一地区、隣接地区\n",
    "    df_add = pd.merge(df_add, df_region,      on=\"id\")\n",
    "    #試合前ランキング\n",
    "    df_add = pd.merge(df_add, df_ranked_team, on=\"id\")\n",
    "    #試合前連勝記録\n",
    "    df_add = pd.merge(df_add, df_series_win,  on=\"id\")\n",
    "    #world cup出場選手数\n",
    "    df_add = pd.merge(df_add, df_wcup,        on=\"id\")\n",
    "    #出場選手の年棒\n",
    "    df_add = pd.merge(df_add, df_money,       on=\"id\")\n",
    "    #同一対戦カードCAGR\n",
    "    df_add = pd.merge(df_add, df_y_cagr_long, on=[\"home\",\"away\",\"year\"])\n",
    "    #同一対戦カード観客動員数平均\n",
    "    df_add = pd.merge(df_add, df_y_mean_long, on=[\"home\",\"away\",\"year\"])\n",
    "    #最終節、優勝決定戦\n",
    "    df_add = df_add.assign(section_last    = df_add.apply(lambda x: 1 if (x.stage==\"Ｊ１\" and x.section in [33,34]) or (x.stage==\"Ｊ２\" and x.section in [41,42]) else 0, axis=1),\n",
    "                           champ_candidate = 0)\n",
    "    df_add.loc[(df_add.stage==\"Ｊ１\")&(df_add.section.isin([33,34]))&(df_add.ranking_home < 5), \"champ_candidate\"] = 1\n",
    "    df_add.loc[(df_add.stage==\"Ｊ２\")&(df_add.section.isin([41,42]))&(df_add.ranking_home < 5), \"champ_candidate\"] = 1\n",
    "    #晴、雨に統一\n",
    "    df_add = re_weather(df_add)\n",
    "    #平日、休日に統一\n",
    "    df_add = re_weekday(df_add)\n",
    "    return df_add\n",
    "\n",
    "# train\n",
    "except_teams = ['ザスパ草津','ＦＣ町田ゼルビア','ガイナーレ鳥取']\n",
    "df_train     = dict_df[\"train\"].query(\"y != 0 & home not in @except_teams & away not in @except_teams\")\n",
    "df_train     = add_cols(df_train)\n",
    "# test\n",
    "except_ids = dict_df['2014_add'].id\n",
    "df_test    = dict_df[\"test\"].query(\"id not in @except_ids\")\n",
    "df_test    = add_cols(df_test)\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "will_drop_cols  = [\"year\",\"match\",\"gameday\",\"time\",\"home\",\"away\",\"stadium\",\"tv\",\n",
    "                   \"address\",\"pref_team\",\"pref_stadium\",\"champ_candidate\",\"local_stadium_opened\",\"champ\",\"cagr\"]\n",
    "std_except_cols = [col for col in df_train if len(df_train[col].unique()) < 3] + [\"id\",\"y\",\"y_mean\",\"cagr\"]\n",
    "\n",
    "std_cols        = list(df_train.select_dtypes(include=[\"int\",\"float\"]).columns)\n",
    "std_cols        = [col for col in std_cols     if col not in will_drop_cols + std_except_cols]\n",
    "dummies_cols    = list(df_train.select_dtypes(include=\"object\").columns)\n",
    "dummies_cols    = [col for col in dummies_cols if col not in will_drop_cols + std_cols]\n",
    "\n",
    "# 標準化\n",
    "df_train, df_test = standardization(df_train, std_cols, df_test)\n",
    "\n",
    "# ダミー変数化\n",
    "df_dummies   = pd.concat([df_train, df_test])\n",
    "df_dummies   = dummies(df_dummies, dummies_cols)\n",
    "df_train, df_test = df_dummies[df_dummies.y.notnull()], df_dummies[df_dummies.y.isnull()]\n",
    "df_train.y   = df_train.y.astype(int)\n",
    "df_test.drop(\"y\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "is_ratio = False\n",
    "#観客動員数の割合\n",
    "if is_ratio:\n",
    "    df_train.y = np.log((df_train.y / df_train.capa) / (1 - (df_train.y / df_train.capa)))\n",
    "x = df_train.drop([\"id\",\"y\"] + std_cols + will_drop_cols, axis=1)\n",
    "y = df_train.y\n",
    "train_x = copy.deepcopy(df_train.query(\"(year == 2013 & '08/01' <= gameday) | year in (2012,2014)\"))\n",
    "train_x.drop([\"id\",\"y\"] + std_cols + will_drop_cols, axis=1, inplace=True)\n",
    "train_y = copy.deepcopy(df_train.query(\"(year == 2013 & '08/01' <= gameday) | year in (2012,2014)\")).y\n",
    "valid_x = copy.deepcopy(df_train.query(\"year == 2013 & gameday <= '08/01'\"))\n",
    "valid_x.drop([\"id\",\"y\"] + std_cols + will_drop_cols, axis=1, inplace=True)\n",
    "valid_y = copy.deepcopy(df_train.query(\"year == 2013 & gameday <= '08/01'\")).y\n",
    "x_test  = df_test.drop([\"id\"] + std_cols + will_drop_cols, axis=1)\n",
    "\n",
    "print(x.shape)\n",
    "x.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "seed = 15\n",
    "#train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "#classifier = SVR(kernel='rbf', gamma=1/2, C=1.0)\n",
    "classifier = xgb.XGBRegressor(gamma=0, learning_rate=0.2, max_depth=3, min_child_weight=20, n_estimators=100, random_state=seed)\n",
    "classifier.fit(x, y)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(classifier.predict(x), y)))\n",
    "print(np.sqrt(mean_squared_error(classifier.predict(train_x), train_y)))\n",
    "print(np.sqrt(mean_squared_error(classifier.predict(valid_x), valid_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_train[\"pred\"] = classifier.predict(x)\n",
    "df_train[\"redi\"] = df_train.y - df_train.pred\n",
    "df_train[[\"id\",\"home\",\"away\",\"year\",\"section\",\"y_mean\",\"y\",\"pred\",\"redi\"]].sort_values(\"redi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pred = classifier.predict(x_test)\n",
    "#pred = 1 / (1 + np.exp(-pred)) * df_test.capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "path = \"../../../../study/jleague/submit/\"\n",
    "df_result = pd.DataFrame({\"id\" : df_test.id, \"result\" : pred})\n",
    "df_result.to_csv(path + \"20180807_5.csv\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_12_14 = copy.deepcopy(pd.concat([dict_df['train'],dict_df['test']]))\n",
    "df_12_14 = pd.merge(df_12_14, dict_df['condition'], on=\"id\")\n",
    "df_12_14 = df_12_14.assign(section = df_12_14.match.apply(lambda x: x[1:x.find(\"節\")]).astype(int),\n",
    "                           weekday = df_12_14.gameday.apply(lambda x: x[x.find(\"(\")+1:x.find(\"(\")+2]))\n",
    "df_12_14 = re_weather(df_12_14)\n",
    "df_12_14 = re_weekday(df_12_14)\n",
    "df_12_14[\"info\"] = df_12_14.weekday + \"-\" + df_12_14.weather + \"-\" + df_12_14.section.astype(str)\n",
    "\n",
    "df_11_info = copy.deepcopy(df_11[[\"home\",\"away\",\"stage\",\"y\",\"section\",\"gameday\",\"weekday\",\"weather\"]])\n",
    "df_11_info = re_weather(df_11_info)\n",
    "df_11_info = re_weekday(df_11_info)\n",
    "df_11_info[\"info\"] = df_11_info.weekday + \"-\" + df_11_info.weather + \"-\" + df_11_info.section.astype(str)\n",
    "\n",
    "df_12_info = df_12_14.query(\"year==2012\")[[\"home\",\"away\",\"stage\",\"y\",\"info\"]]\n",
    "df_13_info = df_12_14.query(\"year==2013\")[[\"home\",\"away\",\"stage\",\"y\",\"info\"]]\n",
    "df_14_info = df_12_14.query(\"year==2014\")[[\"home\",\"away\",\"stage\",\"y\",\"info\"]]\n",
    "\n",
    "df_11_mean = df_11[[\"home\",\"y\"]].groupby(\"home\", as_index=False).y.mean()\n",
    "df_12_mean = df_12_14.query(\"year==2012\")[[\"home\",\"y\"]].groupby(\"home\", as_index=False).y.mean()\n",
    "df_13_mean = df_12_14.query(\"year==2013\")[[\"home\",\"y\"]].groupby(\"home\", as_index=False).y.mean()\n",
    "df_14_mean = df_12_14.query(\"year==2014\")[[\"home\",\"y\"]].groupby(\"home\", as_index=False).y.mean()\n",
    "df_11_mean.rename(columns={\"y\":\"y_mean_11\"}, inplace=True)\n",
    "df_12_mean.rename(columns={\"y\":\"y_mean_12\"}, inplace=True)\n",
    "df_13_mean.rename(columns={\"y\":\"y_mean_13\"}, inplace=True)\n",
    "df_14_mean.rename(columns={\"y\":\"y_mean_14\"}, inplace=True)\n",
    "\n",
    "df_11_12 = pd.merge(df_11_info, df_12_info, on=[\"home\",\"away\"], how=\"outer\", suffixes=(\"_11\",\"_12\"))\n",
    "df_11_13 = pd.merge(df_11_12,   df_13_info, on=[\"home\",\"away\"], how=\"outer\", suffixes=(\"_12\",\"_13\"))\n",
    "df_11_14 = pd.merge(df_11_13,   df_14_info, on=[\"home\",\"away\"], how=\"outer\", suffixes=(\"_13\",\"_14\")).sort_values(by=[\"home\",\"away\"])\n",
    "df_11_14 = pd.merge(df_11_14,   df_11_mean, on= \"home\",         how=\"left\")\n",
    "df_11_14 = pd.merge(df_11_14,   df_12_mean, on= \"home\",         how=\"left\")\n",
    "df_11_14 = pd.merge(df_11_14,   df_13_mean, on= \"home\",         how=\"left\")\n",
    "df_11_14 = pd.merge(df_11_14,   df_14_mean, on= \"home\",         how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_11_14[[\"home\",\"away\",\"y_11\",\"info_11\",\"y_12\",\"info_12\",\"y_13\",\"info_13\",\"y_14\",\"info_14\"]]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
