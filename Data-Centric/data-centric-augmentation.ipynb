{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport copy\nimport glob\nimport json\nimport shutil\nimport random\nimport zipfile\nimport functools\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nfrom torch import nn\nfrom torch.nn import init\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchvision import models\nfrom torchvision import transforms\n\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout,\n    ShiftScaleRotate, CenterCrop, Resize, RandomCrop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mkdir(path):\n    os.makedirs(path, exist_ok=True)\n\ndef get_img(path, return_axis0=False):\n    im_bgr = cv2.imread(path)\n    if return_axis0:\n        return im_bgr[:,:,0]\n    else:\n        return im_bgr\n    \ndef rescale(img):\n    return np.where((0<img)&(img<255), 64, img)\n\ndef show_file_num():\n    res = []\n    for t in AUG_CFG.keys():\n        for d in os.listdir(\"./\"):\n            output_path = f\"./{d}/{t}\"\n            if os.path.exists(output_path):\n                res.append([d, t, len(os.listdir(output_path))])\n    df = pd.DataFrame(res, columns=[\"type\",\"label\",\"cnt\"])\n    df = pd.pivot_table(df, index=\"label\",columns=\"type\",values=\"cnt\",fill_value=0).reset_index()\n    print(\"Noise ratio: \", df.noise.sum() / (df.noise.sum() + df.clean.sum()))\n    return df\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResnetGenerator(nn.Module):\n    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n    \"\"\"\n\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n        \"\"\"Construct a Resnet-based generator\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        \"\"\"\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n        for i in range(n_blocks):       # add ResNet blocks\n\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):  # add upsampling layers\n            mult = 2 ** (n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        \"\"\"Standard forward\"\"\"\n        return self.model(input)\n\nclass ResnetBlock(nn.Module):\n    \"\"\"Define a Resnet block\"\"\"\n\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        \"\"\"Initialize the Resnet block\n        A resnet block is a conv block with skip connections\n        We construct a conv block with build_conv_block function,\n        and implement skip connections in <forward> function.\n        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n        \"\"\"\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        \"\"\"Construct a convolutional block.\n        Parameters:\n            dim (int)           -- the number of channels in the conv layer.\n            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers.\n            use_bias (bool)     -- if the conv layer uses bias or not\n        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n        \"\"\"\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        \"\"\"Forward function (with skip connections)\"\"\"\n        out = x + self.conv_block(x)  # add skip connections\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configuration","metadata":{}},{"cell_type":"code","source":"CFG = {\n    \"input_path\" : \"../input/data-centric-competition-data/data_centric_clean_noise_labeled_data/data_centric_clean_noise_labeled_data/\",\n    \"maug_path\"  : \"../input/data-centric-competition-data/data_centric_make_augmented_data/data_centric_make_augmented_data/\",\n    \"img_size\"   : 128,\n    \"num_workers\": 4,\n    \"device\"     : \"cuda\"\n}\nAUG_CFG = {\n    \"i\"   : [False,False,True, False],\n    \"ii\"  : [False,False,True, False],\n    \"iii\" : [False,False,True, False],\n    \"iv\"  : [False,False,False,True],\n    \"v\"   : [True, False,False,False],\n    \"vi\"  : [False,False,False,True],\n    \"vii\" : [False,False,False,False],\n    \"viii\": [False,False,False,False],\n    \"ix\"  : [False,True, False,False],\n    \"x\"   : [False,True, True, False]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Augmentation methods implmented myself","metadata":{}},{"cell_type":"code","source":"def get_object(path):\n    def sum_with_axis(img, axis):\n        img_sum = img.sum(axis)\n        _min = np.where(img_sum != 0)[0][0] - 1\n        _min = 0 if _min < 0 else _min\n        _max = len(img_sum) - np.where(img_sum[::-1] != 0)[0][0] - 1\n        return _min, _max\n    img_rgb  = cv2.imread(path, cv2.IMREAD_COLOR)\n    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n    _, img_bin = cv2.threshold(img_gray, 50, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n    img_bin  = cv2.medianBlur(img_bin, 3)\n    min_x, max_x = sum_with_axis(img_bin, 0)\n    min_y, max_y = sum_with_axis(img_bin, 1)\n    return img_rgb[min_y:max_y,min_x:max_x]\n\ndef add_black_line(img, line_width=2, add_x=True, add_y=True):\n    img  = copy.deepcopy(img)\n    h, w = img.shape[:2]\n    n_x  = random.choices([1,2], k=1, weights=[0.7,0.3])[0]\n    n_y  = random.choices([1,2], k=1, weights=[0.7,0.3])[0]\n    xs   = random.sample(list(np.arange(w)), n_x)\n    ys   = random.sample(list(np.arange(h)), n_y)\n    if add_x: \n        for x in xs:\n            img[:,x-line_width:x+line_width] = img.min()\n    if add_y:\n        for y in ys:\n            img[y-line_width:y+line_width,:] = img.min()\n    return img\n\ndef add_white_line(img, line_width=2):\n    img   = copy.deepcopy(img)\n    h, w  = img.shape[:2]\n    n_y   = random.choices([10,13,16], k=1)[0]\n    add_y = random.sample(list(np.arange(h)), n_y)\n    for y in add_y:\n        img[y-line_width:y+line_width,:] = img.max()\n    return img\n\ndef add_edge_noise(img):\n    img  = copy.deepcopy(img)\n    prob = np.random.rand(1)[0]\n    if prob < 0.8:\n        n = random.choices([1,2], k=1, weights=[0.7,0.3])[0]\n        add_points = random.sample(list(np.arange(4)), n)\n    else:\n        n = random.choices([3,4], k=1, weights=[0.6,0.4])[0]\n        add_points = random.sample(list(np.arange(4)), n)\n\n    h,  w  = img.shape[:2]\n    hs, ws = int(h*0.2), int(w*0.2)\n    if 0 in add_points:\n        xs = random.sample(list(np.arange(w-ws)), 1)[0]\n        xe = xs + ws\n        ys = 0\n        ye = hs\n        img[ys:ye, xs:xe] = 0\n    if 1 in add_points:\n        xs = random.sample(list(np.arange(w-ws)), 1)[0]\n        xe = xs + ws\n        ys = h - hs\n        ye = h\n        img[ys:ye, xs:xe] = 0\n    if 2 in add_points:\n        ys = random.sample(list(np.arange(h-hs)), 1)[0]\n        ye = ys + hs\n        xs = 0\n        xe = ws\n        img[ys:ye, xs:xe] = 0\n    if 3 in add_points:\n        ys = random.sample(list(np.arange(h-hs)), 1)[0]\n        ye = ys + hs\n        xs = w - ws\n        xe = w\n        img[ys:ye, xs:xe] = 0\n    return img\n\ndef expand_centre(path, n_expand=100):\n    img    = get_object(path)\n    h,w    = img.shape[:2]\n    hc, wc = int(h/2), int(w/2)\n    img_left   = img[:,:wc]\n    img_right  = img[:,wc:]\n    img_centre = img[:,wc-1:wc]\n    img_expand = img_centre\n    for _ in range(n_expand):\n        img_expand = np.hstack([img_expand, img_centre])\n    img_expand = np.hstack([img_left, img_expand, img_right])\n    return img_expand","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define of CycleGan and albumentation","metadata":{}},{"cell_type":"code","source":"def get_generator(path):\n    norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n    generator  = ResnetGenerator(input_nc=1, output_nc=1, ngf=64, norm_layer=norm_layer, use_dropout=False, n_blocks=9)\n    checkpoint = torch.load(path)\n    new_state_dict = OrderedDict()\n    for k, v in checkpoint.items():\n        name = k.replace(\"module.\", \"\") # remove module.\n        new_state_dict[name] = v\n    # load params\n    generator.load_state_dict(new_state_dict, strict=False)\n    return generator\n\ndef get_visual_transforms():\n    return Compose([\n        ShiftScaleRotate(shift_limit=(-0.05, 0.05), scale_limit=(-0.07, 0.07), rotate_limit=(-10,10), p=1)], p=1\n    )\n\ndef get_visual_noise_transforms():\n    return Compose([\n        ShiftScaleRotate(shift_limit=(-0.05, 0.05), scale_limit=(-0.07, 0.07), rotate_limit=(-10,10), p=1),\n        CoarseDropout(max_holes=30, p=1)], p=1\n    )\n\ndef get_noise_transforms():\n    return Compose([\n        CoarseDropout(p=1)], p=1\n    )\n\ndef get_gan_transforms():\n    return Compose([\n        Resize(CFG['img_size'], CFG['img_size']),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)], p=1\n    )\n\ntransforms_v   = get_visual_transforms()\ntransforms_vn  = get_visual_noise_transforms()\ntransforms_n   = get_noise_transforms()\ntransforms_gan = get_gan_transforms()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run an augmentation with competition data","metadata":{}},{"cell_type":"code","source":"!rm -rf ./*\nNUM_CLEAN = len(glob.glob(f\"{CFG['input_path']}clean/**/*\"))\nNUM_NOISE = len(glob.glob(f\"{CFG['input_path']}noise/**/*\"))\nRATIO_NOISE = NUM_NOISE / (NUM_NOISE + NUM_CLEAN)\nRATIO_NOISE = max(0.2, RATIO_NOISE)\nRATIO_NOISE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n!rm -rf ./*\n\nfor t in AUG_CFG.keys():\n    horizontal_flip, vertical_flip, horizontal_vertical_flip, add_with_horizontal_flip = AUG_CFG[t]\n    for c_n in [\"clean\", \"noise\"]:\n        output_path = f\"./{c_n}/{t}/\"\n        mkdir(output_path)\n        input_path  = f\"{CFG['input_path']}{c_n}/{t}\"\n        for p in os.listdir(input_path):\n            img = get_img(f\"{input_path}/{p}\")\n            shutil.copy(f\"{input_path}/{p}\", f\"{output_path}/{p}\")\n            if horizontal_flip:\n                # i, ii, iii, v, x\n                cv2.imwrite(f\"{output_path}hf_{p}\", img[:,::-1,:])\n            if vertical_flip:\n                # i, ii, iii, ix, x\n                cv2.imwrite(f\"{output_path}vf_{p}\", img[::-1,:,:])\n            if horizontal_vertical_flip:\n                # i, ii, iii, x\n                cv2.imwrite(f\"{output_path}hv_{p}\", img[::-1,::-1,:])\n            if add_with_horizontal_flip:\n                # iv -> vi, iv -> vi\n                if   t == \"iv\":\n                    output_path_aw_hf = f\"./{c_n}/vi/\"\n                elif t == \"vi\":\n                    output_path_aw_hf = f\"./{c_n}/iv/\"\n                else:\n                    continue\n                mkdir(output_path_aw_hf)\n                cv2.imwrite(f\"{output_path_aw_hf}aw_hf_{p}\", img[:,::-1,:])\n\nshow_file_num()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nRATIO_CENTRE_WHITE = 0.3\nAUG_PATTERN  = {\"ii\"    : [\"ii\",   300, \"i\",   \"i\"],\n                \"iii-1\" : [\"iii\",  150, \"ii\",  \"i\"],\n                \"iii-2\" : [\"iii\",  150, \"i\",   \"ii\"],\n                \"vi\"    : [\"vi\",   300, \"v\",   \"i\"],\n                \"vii-1\" : [\"vii\",  150, \"v\",   \"ii\"],\n                \"vii-2\" : [\"vii\",  150, \"vi\",  \"i\"],\n                \"viii-1\": [\"viii\", 100, \"v\",   \"iii\"],\n                \"viii-2\": [\"viii\", 100, \"vi\",  \"ii\"],\n                \"viii-3\": [\"viii\", 100, \"vii\", \"i\"],\n                \"iv\"    : [\"iv\",   300, \"i\",   \"v\"],\n                \"ix\"    : [\"ix\",   300, \"i\",   \"x\"]}\n\nfor key, (t, n, l, r) in AUG_PATTERN.items():\n    left_img_path  = os.listdir(CFG['maug_path']+l)\n    left_max_idx   = len(left_img_path)\n    left_sample_idxes  = list(np.arange(left_max_idx))\n    right_img_path = os.listdir(CFG['maug_path']+r)\n    right_max_idx  = len(right_img_path)\n    right_sample_idxes = list(np.arange(right_max_idx))\n    for i in range(n):\n        left  = random.sample(left_sample_idxes,1)[0]\n        left_path  = f\"{CFG['maug_path']}{l}/{left_img_path[left]}\"\n        right = random.sample(right_sample_idxes,1)[0]\n        right_path = f\"{CFG['maug_path']}{r}/{right_img_path[right]}\"\n        \n        left_img  = get_object(left_path)\n        right_img = get_object(right_path)\n        \n        ly, lx  = left_img.shape[:2]\n        ry, rx  = right_img.shape[:2]\n        resizey = min(ly,ry)\n        left_img  = cv2.resize(left_img,  (lx,resizey))\n        right_img = cv2.resize(right_img, (rx,resizey))\n        \n        prob = np.random.rand(1)[0]\n        if prob < RATIO_CENTRE_WHITE:\n            centre_white = np.ones((resizey,100,3), dtype=\"int8\") * 255\n            convert_img  = np.hstack([left_img,centre_white,right_img])\n            if prob < RATIO_CENTRE_WHITE/2:\n                ub_black    = np.zeros((6,convert_img.shape[1],3), dtype=\"int8\")\n                convert_img = np.vstack([ub_black,convert_img[10:-10,:],ub_black])\n        else:\n            convert_img  = np.hstack([left_img,right_img])\n        padx = int(min(lx,rx)/2)\n        pady = int(resizey/2)\n        convert_img = np.pad(convert_img, ((padx, padx), (pady, pady), (0,0)), constant_values=255)\n        cv2.imwrite(f\"./clean/{t}/maug{i}_{os.path.basename(right_path)}\", convert_img)            \n        \nshow_file_num()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n!rm -rf ./clean_aug\n!rm -rf ./noise_aug\n\nNUM_LIMIT_CLEAN  = 1000\nNUM_LIMIT_NOISE  = 500\nLINE_WIDTH_BLACK = 2\nLINE_WIDTH_WHITE = 3\n\nfor t in AUG_CFG.keys():\n    output_path_clean = f\"./clean_aug/{t}/\"\n    output_path_noise = f\"./noise_aug/{t}/\"\n    mkdir(output_path_clean)\n    mkdir(output_path_noise)\n\n    # Run an augmentation with clean data\n    input_path = f\"./clean/{t}\"\n    paths = [f\"{input_path}/{p}\" for p in os.listdir(input_path)]\n    idxes = list(np.arange(len(paths)))\n    for i in range(NUM_LIMIT_CLEAN):\n        prob = np.random.rand(1)[0]\n        s    = random.sample(idxes, 1)[0]\n        path = paths[s]\n        img  = get_img(path)\n        img  = rescale(transforms_v(image=img)['image'])\n        cv2.imwrite(f\"{output_path_clean}aug{i}_{os.path.basename(path)}\", img)\n        \n    # Run an augmentation with noise data\n    input_path_clean = f\"./clean/{t}\"\n    input_path_noise = f\"./noise/{t}\"\n    paths  = [f\"{input_path_clean}/{p}\" for p in os.listdir(input_path_clean)]\n    paths += [f\"{input_path_noise}/{p}\" for p in os.listdir(input_path_noise)]\n    idxes   = list(np.arange(len(paths)))\n    for i in range(NUM_LIMIT_NOISE):\n        prob = np.random.rand(1)[0]\n        s    = random.sample(idxes, 1)[0]\n        path = paths[s]\n        img  = get_img(path)\n        prob = np.random.rand(1)[0]\n        if   prob < 0.20:\n            # Add black dot noise\n            img = transforms_vn(image=img)['image']\n        elif prob < 0.40:\n            # Add edge noise\n            img = transforms_v(image=img)['image']\n            img = add_edge_noise(img)\n        elif prob < 0.60:\n            # Add black line\n            img = transforms_v(image=img)['image']\n            img = add_black_line(img, LINE_WIDTH_BLACK)\n        elif prob < 0.80:\n            # Add black line and white line\n            img = transforms_v(image=img)['image']\n            img = add_black_line(img, LINE_WIDTH_BLACK, add_y=False)\n            img = add_white_line(img, LINE_WIDTH_WHITE)\n        else:\n            # Add white line to delete part of the number\n            img = transforms_v(image=img)['image']\n            img = add_white_line(img, LINE_WIDTH_WHITE)\n        img = rescale(img)\n        cv2.imwrite(f\"{output_path_noise}aug{i}_{os.path.basename(path)}\".replace(\"clean\",\"noise\"), img)\n\nshow_file_num()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nNOISE_TYPE      = 5\nRATIO_ADD_NOISE = 1\n\npath = \"../input/data-centric-cyclegan-models2/\"\ng_paths = [[\"dot\", path+\"noise_dot/generator_5.pth\", 0],\n           [\"bg\",  path+\"noise_background/generator_3.pth\", 0.5]]\nfor t in AUG_CFG.keys():\n    input_path_clean  = f\"./clean/{t}\"\n    output_path_noise = f\"./noise_aug/{t}\"\n    paths = [f\"{input_path_clean}/{p}\" for p in os.listdir(input_path_clean)]\n    random.shuffle(paths)\n    paths = paths[:int(len(os.listdir(output_path_noise))/NOISE_TYPE*RATIO_ADD_NOISE)]\n    \n    for name, gp, rotate_prob in g_paths:\n        generator = get_generator(gp)\n        generator = generator.to(CFG['device'])\n        for p in paths:\n            img  = get_img(p)\n            prob = np.random.rand(1)[0]\n            if prob < rotate_prob:\n                img = img[::-1,::-1,:]\n            img = transforms_gan(image=img)['image'][0,:,:][np.newaxis,np.newaxis,:,:]\n            img = img.to(CFG['device'])\n            g_img = generator(img).detach().to(\"cpu\").numpy()[0,0,:,:]\n            g_img = ((g_img*0.229)+0.485)*255\n            g_img = np.where(g_img>140, 255, g_img)[:,:,np.newaxis]\n            g_img = np.concatenate([g_img, g_img, g_img], axis=2).astype(int)\n            g_img = rescale(g_img)\n            if prob < rotate_prob:\n                g_img = g_img[::-1,::-1,:]\n            cv2.imwrite(f\"{output_path_noise}/cyclegan_{name}_{os.path.basename(p)}\", g_img)\n            \nshow_file_num()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nNUM_VALIDATION   = 300\nRATIO_REAL_NOISE = 0.2\n\nnum_noise_real = int(NUM_VALIDATION * RATIO_NOISE * RATIO_REAL_NOISE)\nnum_noise = int(NUM_VALIDATION * RATIO_NOISE) - num_noise_real\nnum_clean = NUM_VALIDATION - num_noise - num_noise_real\nprint(num_clean, num_noise, num_noise_real)\nfor t in AUG_CFG.keys():\n    mkdir(f\"./val/{t}\")\n    input_path_clean_aug  = f\"./clean_aug/{t}\"\n    input_path_noise_aug  = f\"./noise_aug/{t}\"\n    input_path_noise_real = f\"./noise/{t}\"\n        \n    paths_clean_aug = [f\"{input_path_clean_aug}/{p}\"  for p in os.listdir(input_path_clean_aug)]\n    paths_noise_aug = [f\"{input_path_noise_aug}/{p}\"  for p in os.listdir(input_path_noise_aug)]\n    paths_noise     = [f\"{input_path_noise_real}/{p}\" for p in os.listdir(input_path_noise_real)]\n    random.shuffle(paths_clean_aug)\n    random.shuffle(paths_noise_aug)\n    random.shuffle(paths_noise)\n    clean_train  = paths_clean_aug[:num_clean]\n    noise_train  = paths_noise[:num_noise_real]\n    noise_train += paths_noise_aug[:(num_noise + (num_noise_real - len(noise_train)))]\n    for p in clean_train + noise_train:\n        file_name = os.path.basename(p)\n        shutil.move(p, f\"./val/{t}/{file_name}\")\n\nshow_file_num()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./train\n\nMAX_NUM = 1000 - NUM_VALIDATION\nCLEAN_NOISE_RATIO = 0.7  # Set clean ratio\n\nCLEAN  = int(MAX_NUM * CLEAN_NOISE_RATIO)\nNOISE  = MAX_NUM - CLEAN\nCLEAN -= 1\n\n!rm -rf ./train\n\nfor t in AUG_CFG.keys():\n    mkdir(f\"./train/{t}\")\n    input_path_clean_aug  = f\"./clean_aug/{t}\"\n    input_path_noise_aug  = f\"./noise_aug/{t}\"\n    input_path_noise_real = f\"./noise/{t}\"\n    paths_clean  = [f\"{input_path_clean_aug}/{p}\"  for p in os.listdir(input_path_clean_aug)]\n    paths_noise  = [f\"{input_path_noise_aug}/{p}\"  for p in os.listdir(input_path_noise_aug)]\n    paths_noise += [f\"{input_path_noise_real}/{p}\" for p in os.listdir(input_path_noise_real)]\n    random.shuffle(paths_clean)\n    random.shuffle(paths_noise)\n    clean_train = paths_clean[:CLEAN]\n    noise_train = paths_noise[:NOISE]\n    for p in clean_train + noise_train:\n        shutil.move(p, f\"./train/{t}/{os.path.basename(p)}\")\n        \nshow_file_num()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./submission\nmkdir(\"./submission\")\nmkdir(\"./submission/submission\")\nshutil.copytree(\"./train\", \"./submission/submission/train\")\nshutil.copytree(\"./val\",   \"./submission/submission/val\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive('./submission', 'zip', root_dir='./submission')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./submission/\n!rm -rf ./clean/\n!rm -rf ./noise/\n!rm -rf ./clean_aug/\n!rm -rf ./noise_aug/\n!rm -rf ./train/\n!rm -rf ./val/","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}