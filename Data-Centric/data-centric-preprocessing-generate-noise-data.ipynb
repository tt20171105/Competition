{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport json\nimport random\nimport shutil\nimport functools\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nfrom torch import nn\nfrom torch.nn import init\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchvision import models\nfrom torchvision import transforms\n\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout,\n    ShiftScaleRotate, CenterCrop, Resize)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:12.964886Z","iopub.execute_input":"2021-08-28T13:34:12.965281Z","iopub.status.idle":"2021-08-28T13:34:16.575638Z","shell.execute_reply.started":"2021-08-28T13:34:12.965194Z","shell.execute_reply":"2021-08-28T13:34:16.574762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_img(path, return_axis0=False):\n    im_bgr = cv2.imread(path)\n    if return_axis0:\n        return im_bgr[:,:,0]\n    else:\n        return im_bgr\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:16.577005Z","iopub.execute_input":"2021-08-28T13:34:16.57731Z","iopub.status.idle":"2021-08-28T13:34:16.590424Z","shell.execute_reply.started":"2021-08-28T13:34:16.577277Z","shell.execute_reply":"2021-08-28T13:34:16.589484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configuration","metadata":{}},{"cell_type":"code","source":"CFG = {\n    \"input_path\" : \"../input/data-centric-competition-data/data_centric_clean_noise_pattern_split/data_centric_clean_noise_pattern_split/\",\n    \"save_prev\"  : False,\n    \"save_name\"  : \"\",\n    \"img_size\"   : 128,\n    \"epoch\"      : 15,\n    \"batch_size\" : 32,\n    \"num_workers\": 4,\n    \"device\"     : \"cuda\"\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:35:04.904659Z","iopub.execute_input":"2021-08-28T13:35:04.905025Z","iopub.status.idle":"2021-08-28T13:35:04.911716Z","shell.execute_reply.started":"2021-08-28T13:35:04.90499Z","shell.execute_reply":"2021-08-28T13:35:04.910248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Cyclegan","metadata":{}},{"cell_type":"code","source":"class ResnetGenerator(nn.Module):\n    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n    \"\"\"\n\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n        \"\"\"Construct a Resnet-based generator\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        \"\"\"\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n        for i in range(n_blocks):       # add ResNet blocks\n\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):  # add upsampling layers\n            mult = 2 ** (n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        \"\"\"Standard forward\"\"\"\n        return self.model(input)\n\nclass ResnetBlock(nn.Module):\n    \"\"\"Define a Resnet block\"\"\"\n\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        \"\"\"Initialize the Resnet block\n        A resnet block is a conv block with skip connections\n        We construct a conv block with build_conv_block function,\n        and implement skip connections in <forward> function.\n        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n        \"\"\"\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        \"\"\"Construct a convolutional block.\n        Parameters:\n            dim (int)           -- the number of channels in the conv layer.\n            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers.\n            use_bias (bool)     -- if the conv layer uses bias or not\n        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n        \"\"\"\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        \"\"\"Forward function (with skip connections)\"\"\"\n        out = x + self.conv_block(x)  # add skip connections\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:18.234523Z","iopub.execute_input":"2021-08-28T13:34:18.23494Z","iopub.status.idle":"2021-08-28T13:34:18.270197Z","shell.execute_reply.started":"2021-08-28T13:34:18.234895Z","shell.execute_reply":"2021-08-28T13:34:18.269147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NLayerDiscriminator(nn.Module):\n    \"\"\"Defines a PatchGAN discriminator\"\"\"\n\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n        \"\"\"Construct a PatchGAN discriminator\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator\n            norm_layer      -- normalization layer\n        \"\"\"\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):  # gradually increase the number of filters\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        \"\"\"Standard forward.\"\"\"\n        return self.model(input)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:19.140082Z","iopub.execute_input":"2021-08-28T13:34:19.14039Z","iopub.status.idle":"2021-08-28T13:34:19.152049Z","shell.execute_reply.started":"2021-08-28T13:34:19.140361Z","shell.execute_reply":"2021-08-28T13:34:19.150994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImagePool():\n    \"\"\"This class implements an image buffer that stores previously generated images.\n    This buffer enables us to update discriminators using a history of generated images\n    rather than the ones produced by the latest generators.\n    \"\"\"\n\n    def __init__(self, pool_size):\n        \"\"\"Initialize the ImagePool class\n        Parameters:\n            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created\n        \"\"\"\n        self.pool_size = pool_size\n        if self.pool_size > 0:  # create an empty pool\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        \"\"\"Return an image from the pool.\n        Parameters:\n            images: the latest generated images from the generator\n        Returns images from the buffer.\n        By 50/100, the buffer will return input images.\n        By 50/100, the buffer will return images previously stored in the buffer,\n        and insert the current images to the buffer.\n        \"\"\"\n        if self.pool_size == 0:  # if the buffer size is 0, do nothing\n            return images\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:       # by another 50% chance, the buffer will return the current image\n                    return_images.append(image)\n        return_images = torch.cat(return_images, 0)   # collect all the images and return\n        return return_images","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:19.592688Z","iopub.execute_input":"2021-08-28T13:34:19.593012Z","iopub.status.idle":"2021-08-28T13:34:19.601917Z","shell.execute_reply.started":"2021-08-28T13:34:19.592984Z","shell.execute_reply":"2021-08-28T13:34:19.600939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GANLoss(nn.Module):\n    \"\"\"Define different GAN objectives.\n    The GANLoss class abstracts away the need to create the target label tensor\n    that has the same size as the input.\n    \"\"\"\n\n    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n        \"\"\" Initialize the GANLoss class.\n        Parameters:\n            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n            target_real_label (bool) - - label for a real image\n            target_fake_label (bool) - - label of a fake image\n        Note: Do not use sigmoid as the last layer of Discriminator.\n        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n        \"\"\"\n        super(GANLoss, self).__init__()\n        self.register_buffer('real_label', torch.tensor(target_real_label))\n        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n        self.gan_mode = gan_mode\n        if gan_mode == 'lsgan':\n            self.loss = nn.MSELoss()\n        elif gan_mode == 'vanilla':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif gan_mode in ['wgangp']:\n            self.loss = None\n        else:\n            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n\n    def get_target_tensor(self, prediction, target_is_real):\n        \"\"\"Create label tensors with the same size as the input.\n        Parameters:\n            prediction (tensor) - - tpyically the prediction from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n        Returns:\n            A label tensor filled with ground truth label, and with the size of the input\n        \"\"\"\n\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(prediction)\n\n    def __call__(self, prediction, target_is_real):\n        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n        Parameters:\n            prediction (tensor) - - tpyically the prediction output from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n        Returns:\n            the calculated loss.\n        \"\"\"\n        if self.gan_mode in ['lsgan', 'vanilla']:\n            target_tensor = self.get_target_tensor(prediction, target_is_real)\n            loss = self.loss(prediction, target_tensor)\n        elif self.gan_mode == 'wgangp':\n            if target_is_real:\n                loss = -prediction.mean()\n            else:\n                loss = prediction.mean()\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:20.00663Z","iopub.execute_input":"2021-08-28T13:34:20.00696Z","iopub.status.idle":"2021-08-28T13:34:20.017288Z","shell.execute_reply.started":"2021-08-28T13:34:20.006932Z","shell.execute_reply":"2021-08-28T13:34:20.016383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CycleGan(nn.Module):\n    \n    def __init__(self, \n                 generator_a, generator_b, discriminator_a, discriminator_b,  \n                 discriminator_loss, lambda_a, lambda_b, device):\n        super(CycleGan, self).__init__()\n        self.generator_a = generator_a\n        self.generator_b = generator_b\n        self.discriminator_a = discriminator_a\n        self.discriminator_b = discriminator_b\n        self.discriminator_loss = discriminator_loss\n        self.reconstruct_loss   = nn.L1Loss()\n        self.device = device\n        \n        self.image_pool_a = ImagePool(50)\n        self.image_pool_b = ImagePool(50)\n        \n        self.lambda_a = lambda_a\n        self.lambda_b = lambda_b\n        \n        self.real_images_a = None\n        self.real_images_b = None\n        self.labels_a = None\n        self.labels_b = None\n        self.fake_images_a = None\n        self.fake_images_b = None\n        self.rec_images_a  = None\n        self.rec_images_b  = None\n        self.generator_a   = torch.nn.DataParallel(self.generator_a)\n        self.generator_b   = torch.nn.DataParallel(self.generator_b)\n        self.discriminator_a = torch.nn.DataParallel(self.discriminator_a)\n        self.discriminator_b = torch.nn.DataParallel(self.discriminator_b)\n        self.to(device)\n        \n    def forward(self):\n        self.fake_images_a = self.generator_a(self.real_images_b)\n        self.fake_images_b = self.generator_b(self.real_images_a)\n        self.rec_images_a  = self.generator_a(self.fake_images_b)\n        self.rec_images_b  = self.generator_b(self.fake_images_a)\n    \n    @staticmethod\n    def set_requires_grad(nets, requires_grad=False):\n        \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n        Parameters:\n            nets (network list)   -- a list of networks\n            requires_grad (bool)  -- whether the networks require gradients or not\n        \"\"\"\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n                    \n                    \n    def generator_step(self):\n        CycleGan.set_requires_grad([self.discriminator_a, self.discriminator_b], False)\n        \n        loss_a  = self.discriminator_loss(self.discriminator_a(self.fake_images_a), True)\n        loss_b  = self.discriminator_loss(self.discriminator_b(self.fake_images_b), True)\n        cycle_a = self.reconstruct_loss(self.rec_images_a, self.real_images_a)*self.lambda_a\n        cycle_b = self.reconstruct_loss(self.rec_images_b, self.real_images_b)*self.lambda_b\n        \n        loss = loss_a + loss_b + cycle_a + cycle_b\n        loss.backward()\n        CycleGan.set_requires_grad([self.discriminator_a, self.discriminator_b], True)\n        return loss, loss_a, loss_b, cycle_a, cycle_b\n        \n    def discriminator_step(self):\n        pred_real_a = self.discriminator_a(self.real_images_a)\n        loss_real_a = self.discriminator_loss(pred_real_a, True)\n        fake_images_a = self.image_pool_a.query(self.fake_images_a).detach()\n        pred_fake_a = self.discriminator_a(fake_images_a)\n        loss_fake_a = self.discriminator_loss(pred_fake_a, False)\n        \n        pred_real_b = self.discriminator_b(self.real_images_b)\n        loss_real_b = self.discriminator_loss(pred_real_b, True)\n        fake_images_b = self.image_pool_b.query(self.fake_images_b).detach()\n        pred_fake_b = self.discriminator_b(fake_images_b)\n        loss_fake_b = self.discriminator_loss(pred_fake_b, False)\n        \n        loss = (loss_real_a + loss_fake_a)/2 + (loss_real_b + loss_fake_b)/2\n        loss.backward()\n        return loss, loss_real_a, loss_fake_a, (loss_real_a + loss_fake_a)/2, loss_real_b, loss_fake_b, (loss_real_b+loss_fake_b)/2\n    \n    def set_input(self, images_a, images_b):\n        self.real_images_a = images_a.to(self.device)\n        self.real_images_b = images_b.to(self.device)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:20.348428Z","iopub.execute_input":"2021-08-28T13:34:20.348759Z","iopub.status.idle":"2021-08-28T13:34:20.367744Z","shell.execute_reply.started":"2021-08-28T13:34:20.348731Z","shell.execute_reply":"2021-08-28T13:34:20.366395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LossAverager:\n    \n    def __init__(self, prefix):\n        self.prefix = prefix\n        self.generator_loss   = []\n        self.generator_loss_a = []\n        self.generator_loss_b = []\n        self.cycle_a = []\n        self.cycle_b = []\n        self.discriminator_loss   = []\n        self.loss_real_a = []\n        self.loss_fake_a = []\n        self.discriminator_loss_a = []\n        self.loss_real_b = []\n        self.loss_fake_b = []\n        self.discriminator_loss_b = []\n    \n    def append(self, generator_loss, generator_loss_a, generator_loss_b, cycle_a, cycle_b, discriminator_loss, loss_real_a, loss_fake_a, discriminator_loss_a, loss_real_b, loss_fake_b, discriminator_loss_b):\n        self.generator_loss.append(generator_loss.item())\n        self.generator_loss_a.append(generator_loss_a.item())\n        self.generator_loss_b.append(generator_loss_b.item())\n        self.cycle_a.append(cycle_a.item())\n        self.cycle_b.append(cycle_b.item())\n        self.discriminator_loss.append(discriminator_loss.item())\n        self.loss_real_a.append(loss_real_a.item())\n        self.loss_fake_a.append(loss_fake_a.item())\n        self.discriminator_loss_a.append(discriminator_loss_a.item())\n        self.loss_real_b.append(loss_real_b.item())\n        self.loss_fake_b.append(loss_fake_b.item())\n        self.discriminator_loss_b.append(discriminator_loss_b.item())\n\n    def average(self):\n        metric = {}\n        for key, value in self.__dict__.items():\n            if isinstance(value, list):\n                metric[self.prefix+'/'+key] = sum(value)/len(value)\n        return metric","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:20.712256Z","iopub.execute_input":"2021-08-28T13:34:20.712559Z","iopub.status.idle":"2021-08-28T13:34:20.723544Z","shell.execute_reply.started":"2021-08-28T13:34:20.712532Z","shell.execute_reply":"2021-08-28T13:34:20.722557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def warmup_linear_decay(step):\n    if step < WARM_UP_STEP:\n        return 1.0\n    else:\n        return (train_steps-step)/(train_steps-WARM_UP_STEP)\n\ndef init_weight(net, init_gain):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            init.normal_(m.weight.data, 0.0, init_gain)\n            if hasattr(m, 'bias'):\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            init.normal_(m.weight.data, 1.0, init_gain)\n            init.constant_(m.bias.data, 0.0)\n    net.apply(init_func)\n    \ndef train_step(model, a_iter, b_iter, generator_optimizer, discriminator_optimizer, generator_scheduler, discriminator_scheduler, device):\n    a_image = next(a_iter)\n    b_image = next(b_iter)\n    a_image = a_image.to(device)\n    b_image = b_image.to(device)\n    model.set_input(a_image, b_image)\n    model.forward()\n    generator_optimizer.zero_grad()\n    generator_loss, generator_loss_a, generator_loss_b, cycle_a, cycle_b = model.generator_step()\n    generator_optimizer.step()\n    discriminator_optimizer.zero_grad()\n    discriminator_loss, loss_real_a, loss_fake_a, discriminator_loss_a, loss_real_b, loss_fake_b, discriminator_loss_b = model.discriminator_step()\n    discriminator_optimizer.step()\n    generator_scheduler.step()\n    discriminator_scheduler.step()\n    return a_image, (generator_loss, generator_loss_a, generator_loss_b, cycle_a, cycle_b, discriminator_loss, loss_real_a, loss_fake_a, discriminator_loss_a, loss_real_b, loss_fake_b, discriminator_loss_b)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:21.273566Z","iopub.execute_input":"2021-08-28T13:34:21.273871Z","iopub.status.idle":"2021-08-28T13:34:21.284818Z","shell.execute_reply.started":"2021-08-28T13:34:21.273842Z","shell.execute_reply":"2021-08-28T13:34:21.283786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_model():\n    norm_layer  = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n    generator_a = ResnetGenerator(input_nc=1, output_nc=1, ngf=64, norm_layer=norm_layer, use_dropout=False, n_blocks=9)\n    generator_b = ResnetGenerator(input_nc=1, output_nc=1, ngf=64, norm_layer=norm_layer, use_dropout=False, n_blocks=9)\n    discriminator_a = NLayerDiscriminator(input_nc=1, ndf=64, n_layers=3, norm_layer=norm_layer)\n    discriminator_b = NLayerDiscriminator(input_nc=1, ndf=64, n_layers=3, norm_layer=norm_layer)\n    init_weight(generator_a, 0.02)\n    init_weight(generator_b, 0.02)\n    init_weight(discriminator_a, 0.02)\n    init_weight(discriminator_b, 0.02)\n    discriminator_loss      = GANLoss('lsgan', target_real_label=1.0, target_fake_label=0.0)\n    model = CycleGan(generator_a=generator_a,\n                     generator_b=generator_b,\n                     discriminator_a=discriminator_a,\n                     discriminator_b=discriminator_b,\n                     discriminator_loss=discriminator_loss,\n                     lambda_a=10.0,\n                     lambda_b=10.0,\n                     device=CFG[\"device\"])\n    return model, generator_a, generator_b, discriminator_a, discriminator_b","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:34:36.141538Z","iopub.execute_input":"2021-08-28T13:34:36.141867Z","iopub.status.idle":"2021-08-28T13:34:36.150371Z","shell.execute_reply.started":"2021-08-28T13:34:36.141838Z","shell.execute_reply":"2021-08-28T13:34:36.149286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DCDataset(Dataset):\n\n    def __init__(self, df, transforms=None):\n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        path = self.df.iloc[index]['path']\n        img  = get_img(path)\n        if self.transforms:\n            img = self.transforms(image=img)['image'][0,:,:][np.newaxis,:,:]\n        return img\n    \ndef get_train_transforms():\n    return Compose([\n        ShiftScaleRotate(shift_limit=(-0.05, 0.05), scale_limit=(-0.07, 0.07), rotate_limit=(-10,10), p=0.7),\n        Resize(CFG['img_size'], CFG['img_size']),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)], p=1\n    )","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:37:49.339106Z","iopub.execute_input":"2021-08-28T13:37:49.339428Z","iopub.status.idle":"2021-08-28T13:37:49.350382Z","shell.execute_reply.started":"2021-08-28T13:37:49.339388Z","shell.execute_reply":"2021-08-28T13:37:49.349529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(generator, img):\n    if img.ndim == 3:\n        img = img[np.newaxis,:,:,:]\n    return generator(img.to(\"cuda\")).detach().to(\"cpu\").numpy()\n\ndef show_images(generator, imgs):\n    c = 6\n    r = 3\n    fig, axes = plt.subplots(nrows=r, ncols=c, figsize=(14, 5))\n    cnt = 0\n    for i in range(r):\n        for j in range(0,c,2):\n            if r*c < cnt:\n                continue\n            img = imgs[cnt]\n            axes[i, j].imshow(img[0].to(\"cpu\"), cmap=\"gray\")\n            axes[i, j+1].imshow(generate(generator, img)[0][0], cmap=\"gray\")\n            cnt += 1\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:37:54.114216Z","iopub.execute_input":"2021-08-28T13:37:54.114562Z","iopub.status.idle":"2021-08-28T13:37:54.123732Z","shell.execute_reply.started":"2021-08-28T13:37:54.114531Z","shell.execute_reply":"2021-08-28T13:37:54.12271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparation of data and train CycleGan models","metadata":{}},{"cell_type":"code","source":"if CFG[\"save_prev\"]:\n    for d in [\"noise_background\",\"noise_dot\",\"noise_line\"]:\n        !rm -rf ./{CFG['save_name']}_{d}/\n        shutil.copytree(f\"../input/data-centric-cyclegan-models/{d}/\",\n                        f\"./{CFG['save_name']}_{d}/\")","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:35:24.277039Z","iopub.execute_input":"2021-08-28T13:35:24.277379Z","iopub.status.idle":"2021-08-28T13:35:27.780218Z","shell.execute_reply.started":"2021-08-28T13:35:24.27733Z","shell.execute_reply":"2021-08-28T13:35:27.779151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 2\ndf_clean  = pd.DataFrame(glob.glob(CFG['input_path']+\"clean/*.png\", recursive=True), columns=[\"path\"])\nfor _ in range(N):\n    df_clean = df_clean.append(df_clean)\ndf_clean  = df_clean.reset_index(drop=True)\ndf_noises = {}\nfor n in [\"noise_background\",\"noise_dot\"]:\n    df_noise = pd.DataFrame(glob.glob(CFG['input_path']+n+\"/*.png\", recursive=True), columns=[\"path\"])\n    df = pd.DataFrame()\n    while True:\n        df = df.append(df_noise)\n        if df_clean.shape[0] < df.shape[0]:\n            break\n    df_noise = df.reset_index(drop=True)\n    print(n, df_clean.shape, df_noise.shape)\n    df_noises[n] = df_noise","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:35:37.529622Z","iopub.execute_input":"2021-08-28T13:35:37.52997Z","iopub.status.idle":"2021-08-28T13:35:38.801004Z","shell.execute_reply.started":"2021-08-28T13:35:37.529931Z","shell.execute_reply":"2021-08-28T13:35:38.800155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key, df_noise in df_noises.items():\n    print(key)\n    clean_ds = DCDataset(df_clean, transforms=get_train_transforms())\n    noise_ds = DCDataset(df_noise, transforms=get_train_transforms())\n    clean_loader = torch.utils.data.DataLoader(clean_ds,\n                                               batch_size=CFG['batch_size'],\n                                               pin_memory=False,\n                                               drop_last=False,\n                                               shuffle=True,\n                                               num_workers=CFG['num_workers'])\n    noise_loader = torch.utils.data.DataLoader(noise_ds,\n                                               batch_size=CFG['batch_size'],\n                                               pin_memory=False,\n                                               drop_last=False,\n                                               shuffle=True,\n                                               num_workers=CFG['num_workers'])\n    \n    model, generator_a, generator_b, discriminator_a, discriminator_b = make_model()\n    generator_optimizer     = torch.optim.Adam(itertools.chain(generator_a.parameters(), generator_b.parameters()), lr=0.0002, betas=(0.5, 0.999))\n    discriminator_optimizer = torch.optim.Adam(itertools.chain(discriminator_a.parameters(), discriminator_b.parameters()), lr=0.0002, betas=(0.5, 0.999))\n    \n    num_step_per_epoch = len(clean_loader)//CFG[\"epoch\"]\n    train_steps  = num_step_per_epoch*CFG[\"epoch\"]\n    WARM_UP_STEP = train_steps*0.5\n    generator_scheduler     = torch.optim.lr_scheduler.LambdaLR(generator_optimizer, warmup_linear_decay)\n    discriminator_scheduler = torch.optim.lr_scheduler.LambdaLR(discriminator_optimizer, warmup_linear_decay)\n    \n    log = []\n    clean_loader_iter = iter(clean_loader)\n    noise_loader_iter = iter(noise_loader)\n    for epoch in range(CFG[\"epoch\"]):\n        model.train()\n        loss_averager = LossAverager('train')\n        for i in tqdm(range(num_step_per_epoch)):\n            img, losses = train_step(model, clean_loader_iter, noise_loader_iter,\n                                     generator_optimizer, discriminator_optimizer,\n                                     generator_scheduler, discriminator_scheduler, CFG[\"device\"])\n            loss_averager.append(*losses)\n        # Show sample images\n        show_images(model.generator_b, img)\n        \n        metric = loss_averager.average()\n        metric['epoch'] = epoch\n        model.eval()\n        log.append(metric)\n        if not os.path.exists(f\"./{key}\"):\n            os.mkdir(f\"./{key}\")\n        torch.save(model.generator_b.state_dict(), f'./{key}/generator_{epoch}.pth')\n        with open(f'log_{key}.json', 'w') as fout:\n            json.dump(log , fout, indent=4)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:37:58.68503Z","iopub.execute_input":"2021-08-28T13:37:58.685336Z"},"trusted":true},"execution_count":null,"outputs":[]}]}