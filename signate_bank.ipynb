{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling as pdp\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.svm import LinearSVC, libsvm, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# エグゼクティブサマリ\n",
    "- モデル精度  \n",
    "  mさん：0.89722（決定木）   \n",
    "  nさん ：0.917   (バギング)  \n",
    "  hさん ：0.91851（ロジスティック＆決定木）  \n",
    "  ttさん ：0.93800 (勾配ブースティング＆アダブースト＆ランダムフォレスト＆Extra-trees)  \n",
    "  全員のスタッキング：**0.93549**  \n",
    "  \n",
    "  \n",
    "- チームの方針  \n",
    "  それぞれがベストと思うモデルを持ち寄る。  \n",
    "  集まったモデルをスタッキングしてメタモデルを作成し、精度検証する。  \n",
    "  \n",
    "  \n",
    "- 疑問点  \n",
    "  hさんのは２種類のモデルをスタッキングしたメタモデルの予測値を使用。（予測値１つ）  \n",
    "  ttさんのは４種類のモデルそのまま使用。（予測値４つ）  \n",
    "  hさんの分に関してはスタッキングのスタッキングになっている。  \n",
    "  こういうのはあまりしない？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# 自作の説明変数を作成する関数\n",
    "def create_explain_variable(df):\n",
    "    df_add_var = copy.deepcopy(df)\n",
    "    # 年齢を年代に変換\n",
    "    classes = [15,25,30,35,40,45,50,55,60,120]\n",
    "    df_add_var[\"ageclass\"] = pd.cut(df_add_var.age, classes, labels=np.arange(9)).astype(int)\n",
    "    # 年齢を50歳で折り返して線形にする\n",
    "    df_add_var[\"age_abs\"] = np.abs(50 - df_add_var.age)\n",
    "    # 経過日数を２か月、６か月、１年に分割する\n",
    "    classes = [-2,0,60,120,180,360,720,2000]\n",
    "    df_add_var[\"pdaysclass\"] = pd.cut(df_add_var.pdays, classes, labels=np.arange(7)).astype(int)\n",
    "    # 新規顧客フラグ\n",
    "    df_add_var[\"new_customers\"] = 0\n",
    "    df_add_var.loc[df_add_var.previous==0, \"new_customers\"] = 1\n",
    "    # 最終接触日を10日未満、10～20日、20日以降に分割し、月と結合する\n",
    "    df_add_var[\"daysplit\"] = 0\n",
    "    df_add_var.loc[(10 < df_add_var.day)&(df_add_var.day <= 20), \"daysplit\"] = 1\n",
    "    df_add_var.loc[(20 <= df_add_var.day), \"daysplit\"] = 2\n",
    "    df_add_var[\"md\"] = df_add_var.month + df_add_var.daysplit.astype(str)\n",
    "    # 21歳から働き始めたとみなして、平均年間貯蓄額を算出（21歳以下、学生は0とみなす）\n",
    "    df_add_var[\"mean_balance\"] = df_add_var.balance / (df_add_var.age - 21)\n",
    "    df_add_var.loc[(df_add_var.age <= 21)|(df_add_var.job==\"student\"), \"mean_balance\"] = 0\n",
    "    # 平均接触回数\n",
    "    df_add_var[\"mean_meet_num\"] = df_add_var.previous / df_add_var.pdays\n",
    "    df_add_var.loc[df_add_var.pdays==-1, \"mean_meet_num\"] = 0\n",
    "    return df_add_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# カテゴリデータのダミー変数化\n",
    "def val2cate(df1, df2):\n",
    "    df_train = copy.deepcopy(df1)\n",
    "    df_test  = copy.deepcopy(df2)\n",
    "    for col in [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\",\"md\"]:\n",
    "        if col not in df_train.columns: continue\n",
    "        for idx, val in enumerate(sorted(df_train[col].unique())):\n",
    "            df_train.loc[df_train[col]==val, col] = idx\n",
    "            df_test.loc[df_test[col]==val, col]   = idx\n",
    "    return df_train, df_test\n",
    "# 数値データの標準化（しなくてよいけど一応）\n",
    "def val2num(df1, df2):\n",
    "    df_train = copy.deepcopy(df1)\n",
    "    df_test  = copy.deepcopy(df2)\n",
    "    for col in [\"age\",\"balance\",\"duration\",\"campaign\",\"pdays\"]:\n",
    "        if col not in df_train.columns: continue\n",
    "        train_mean = df_train[col].mean()\n",
    "        train_std  = df_train[col].std()\n",
    "        df_train[col] = df_train[col].apply(lambda x: (x - train_mean) / train_std)\n",
    "        df_test[col]  = df_test[col].apply(lambda x: (x - train_mean) / train_std)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "path = \"../../../../study/bank/motodata/\"\n",
    "df_train = pd.read_csv(path + \"train.csv\")\n",
    "df_test  = pd.read_csv(path + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# データ観察（pdays と previousはピアソン、スピアマン高めなのでpdaysをあとで削除）\n",
    "pdp.ProfileReport(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# 外れ値データを削除（全員のモデルを足すときは実行していない）\n",
    "dict_del = {\"previous\":[275],\n",
    "            \"duration\":[4918],\n",
    "            \"balance\" :[-6847,102127]}\n",
    "for col, vals in dict_del.items():\n",
    "    for val in vals:\n",
    "        del_idx = df_train[df_train[col]==val].index\n",
    "        df_train.drop(del_idx, inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# 自作変数\n",
    "df_train_add_var = create_explain_variable(df_train)\n",
    "df_test_add_var  = create_explain_variable(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# 不要な列を削除\n",
    "drop_cols = [\"id\",\"pdays\"]\n",
    "df_train_add_var.drop(drop_cols, axis=1, inplace=True)\n",
    "df_test_add_var.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ダミー変数化、標準化\n",
    "df_train_add_var, df_test_add_var = val2cate(df_train_add_var, df_test_add_var)\n",
    "df_train_add_var, df_test_add_var = val2num(df_train_add_var, df_test_add_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# スタッキングする関数\n",
    "def calc_proba(proba):\n",
    "    return 1 - proba[:,0]\n",
    "def stacking(df_train, df_test, clf, name, seed=15, cv=4):\n",
    "    statime = datetime.now()\n",
    "    df_auc  = df_train_pred = pd.DataFrame()\n",
    "    x, y    = df_train.drop(\"y\", axis=1), df_train.y\n",
    "    # train\n",
    "    k   = 1\n",
    "    skf = StratifiedKFold(n_splits=cv, random_state=seed)\n",
    "    for train, valid in skf.split(x, y):\n",
    "        clf.fit(x.iloc[train,:], y[train])\n",
    "        train_pred, valid_pred = clf.predict_proba(x.iloc[train,:]), clf.predict_proba(x.iloc[valid,:])\n",
    "        train_pred, valid_pred = calc_proba(train_pred),             calc_proba(valid_pred)\n",
    "        auc     = pd.DataFrame({\"k\" : [k],\n",
    "                                \"train_\"+name : roc_auc_score(y[train], train_pred),\n",
    "                                \"valid_\"+name : roc_auc_score(y[valid], valid_pred)})\n",
    "        df_auc  = df_auc.append(auc, ignore_index=True)\n",
    "        df_pred = pd.DataFrame({\"idx\" : valid, name : valid_pred, \"y\" : y[valid]})\n",
    "        df_train_pred = df_train_pred.append(df_pred, ignore_index=True)\n",
    "        k += 1\n",
    "    # test\n",
    "    clf.fit(x, y)\n",
    "    all_pred, test_pred = clf.predict_proba(x), clf.predict_proba(df_test)\n",
    "    all_pred, test_pred = calc_proba(all_pred), calc_proba(test_pred)\n",
    "    auc    = pd.DataFrame({\"k\" : [\"all\"],\n",
    "                           \"train_\"+name : roc_auc_score(y, all_pred)})\n",
    "    df_auc = df_auc.append(auc, ignore_index=True)\n",
    "    df_test_pred = pd.DataFrame({\"idx\" : df_test.index, name : test_pred})\n",
    "    print(\"clf:%s time:%s end\" % (name, datetime.now() - statime))\n",
    "    return df_train_pred, df_test_pred, df_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# クロスバリデーション k=4 でスタッキング（ハイパーパラメータはGridSearchで決定）\n",
    "seed=15\n",
    "# 勾配ブースティングでモデル作成\n",
    "classifier = xgb.XGBClassifier(learning_rate=0.01, max_depth=8, min_child_weight=6, n_estimators=3000, random_state=seed)\n",
    "df_train_xgb, df_test_xgb, df_auc_xgb = stacking(df_train_add_var, df_test_add_var, classifier, \"xgb\")\n",
    "# アダブーストでモデル作成\n",
    "classifier = AdaBoostClassifier(learning_rate=0.9, n_estimators=3000, random_state=seed)\n",
    "df_train_ab,  df_test_ab,  df_auc_ab  = stacking(df_train_add_var, df_test_add_var, classifier, \"ab\")\n",
    "# ランダムフォレストでモデル作成\n",
    "classifier = RandomForestClassifier(max_depth=10, min_samples_leaf=2, max_features=\"sqrt\", n_estimators=3000, n_jobs=-1, random_state=seed)\n",
    "df_train_rf,  df_test_rf,  df_auc_rf  = stacking(df_train_add_var, df_test_add_var, classifier, \"rf\")\n",
    "# ExtraTreesでモデル作成\n",
    "classifier = ExtraTreesClassifier(max_depth=8, min_samples_leaf=1, n_estimators=1000, n_jobs=-1, random_state=seed)\n",
    "df_train_et,  df_test_et,  df_auc_et  = stacking(df_train_add_var, df_test_add_var, classifier, \"et\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 各モデルのAUC\n",
    "#  xgb : 勾配ブースティング\n",
    "#  ab  : アダブースト\n",
    "#  rf  : ランダムフォレスト\n",
    "#  et  : Extra-trees\n",
    "pd.concat([df_auc_xgb, df_auc_ab, df_auc_rf, df_auc_et], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "df_train_sta = pd.merge(df_train_rf,  df_train_xgb, on=[\"idx\",\"y\"])\n",
    "df_train_sta = pd.merge(df_train_sta, df_train_et,  on=[\"idx\",\"y\"])\n",
    "df_train_sta = pd.merge(df_train_sta, df_train_ab,  on=[\"idx\",\"y\"])\n",
    "df_train_sta = df_train_sta[[\"idx\", \"xgb\", \"ab\", \"rf\", \"et\", \"y\"]]\n",
    "# test\n",
    "df_test_sta  = pd.merge(df_test_rf,  df_test_xgb, on=\"idx\")\n",
    "df_test_sta  = pd.merge(df_test_sta, df_test_et,  on=\"idx\")\n",
    "df_test_sta  = pd.merge(df_test_sta, df_test_ab,  on=\"idx\")\n",
    "df_test_sta  = df_test_sta[[\"idx\", \"xgb\", \"ab\", \"rf\", \"et\"]]\n",
    "print(\"train shape :\", df_train_sta.shape)\n",
    "print(\"test  shape :\", df_test_sta.shape)\n",
    "df_train_sta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# チームのみんなが作ったモデルを加える\n",
    "path = \"../../../../study/bank/team_predict/\"\n",
    "df_train_m = pd.read_csv(path+\"train_miyata.csv\",   names=[\"idx\",\"pred_m\"])\n",
    "df_train_n = pd.read_csv(path+\"train_nakamura.csv\", names=[\"idx\",\"pred_n\"])\n",
    "df_train_h = pd.read_csv(path+\"train_hayashi.csv\",  names=[\"idx\",\"pred_h\"])\n",
    "df_test_m  = pd.read_csv(path+\"test_miyata.csv\",    names=[\"idx\",\"pred_m\"])\n",
    "df_test_n  = pd.read_csv(path+\"test_nakamura.csv\",  names=[\"idx\",\"pred_n\"])\n",
    "df_test_h  = pd.read_csv(path+\"test_hayashi.csv\",   names=[\"idx\",\"pred_h\"])\n",
    "print(\"miyata   train/test model shape :\", df_train_m.shape, df_test_m.shape)\n",
    "print(\"nakamura train/test model shape :\", df_train_n.shape, df_test_n.shape)\n",
    "print(\"hayashi  train/test model shape :\", df_train_h.shape, df_test_h.shape)\n",
    "df_train_sta.idx = df_train_sta.idx+1\n",
    "df_train_sta = pd.merge(df_train_sta, df_train_m, on=\"idx\")\n",
    "df_train_sta = pd.merge(df_train_sta, df_train_n, on=\"idx\")\n",
    "df_train_sta = pd.merge(df_train_sta, df_train_h, on=\"idx\")\n",
    "df_train_sta = df_train_sta[[\"idx\", \"xgb\", \"ab\", \"rf\", \"et\", \"pred_m\", \"pred_n\", \"pred_h\", \"y\"]]\n",
    "df_test_sta.idx  = df_test_sta.idx+1\n",
    "df_test_sta  = pd.merge(df_test_sta,  df_test_m, on=\"idx\")\n",
    "df_test_sta  = pd.merge(df_test_sta,  df_test_n, on=\"idx\")\n",
    "df_test_sta  = pd.merge(df_test_sta,  df_test_h, on=\"idx\")\n",
    "df_test_sta  = df_test_sta[[\"idx\", \"xgb\", \"ab\", \"rf\", \"et\", \"pred_m\", \"pred_n\", \"pred_h\"]]\n",
    "df_train_sta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x    = df_train_sta.drop([\"idx\",\"y\"], axis=1)\n",
    "y    = df_train_sta.y\n",
    "test = df_test_sta.drop(\"idx\", axis=1)\n",
    "# meta model（ハイパーパラメータはGridSearchで決定）\n",
    "classifier = xgb.XGBClassifier(gamma=3, learning_rate=0.1, max_depth=4, min_child_weight=6, n_estimators=500, random_state=seed)\n",
    "classifier.fit(x, y)\n",
    "train_pred = calc_proba(classifier.predict_proba(x))\n",
    "print(\"train auc:\", roc_auc_score(y, train_pred))\n",
    "test_pred  = calc_proba(classifier.predict_proba(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame({\"idx\"    : np.arange(1, df_test_sta.shape[0]+1),\n",
    "                          \"result\" : test_pred})\n",
    "path = \"../../../../study/bank/submit/\"\n",
    "df_result.to_csv(path + \"result_20180627_2.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここから説明不要  \n",
    "GridSearchでハイパーパラメータを決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# cross validation\n",
    "seed  = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#勾配ブースティング\n",
    "classifier = xgb.XGBClassifier(random_state=seed)\n",
    "parameters = {'n_estimators'     : [500,1000],\n",
    "              'learning_rate'    : [0.1,0.3,0.5,0.9], \n",
    "              'max_depth'        : [4,6],\n",
    "              'min_child_weight' : [4,6,10],\n",
    "              'gamma'            : [0,3,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#extra trees\n",
    "classifier = ExtraTreesClassifier(random_state=seed)\n",
    "parameters = {'n_estimators'     : [500, 1000],\n",
    "              'max_depth'        : [6, 8, 10],\n",
    "              'min_samples_leaf' : [1, 3, 5, 9, 17],\n",
    "              'min_samples_split': [0.1, 0.3, 0.5, 0.7, 0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#ランダムフォレスト\n",
    "classifier = RandomForestClassifier(random_state=seed)\n",
    "parameters = {'n_estimators'     : [500, 1000],\n",
    "              'max_depth'        : [4, 6, 8],\n",
    "              'min_samples_leaf' : [100, 200, 300],\n",
    "              'min_samples_split': [0.1, 0.3, 0.5, 0.7, 0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#k最近傍法\n",
    "classifier = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':[1,2,3,4,5],\n",
    "              \"leaf_size\"  :[1,2,3,4,5,10],\n",
    "              \"p\"          :[1,2,3,4,5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#SVM\n",
    "classifier = SVC(random_state=seed)\n",
    "parameters = {'C'    :np.logspace(-4, 4, 3),\n",
    "              'gamma':np.logspace(-4, 4, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#アダブースト\n",
    "classifier = AdaBoostClassifier(random_state=seed)\n",
    "parameters = {'n_estimators'    :[3000],\n",
    "              'learning_rate'   :[0.1,0.2,0.3,0.5,0.7,0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#ロジスティック回帰\n",
    "classifier = LogisticRegression(random_state=seed)\n",
    "parameters = {'C' : [0.1,1,10,20,30,40,50,60,70,80,90,100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = df_train_add_var.drop(\"y\", axis=1)\n",
    "y = df_train_add_var.y\n",
    "train_X, test_X, train_y, test_y = train_test_split(x, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "cv = GridSearchCV(classifier, parameters, cv=4, scoring=\"roc_auc\", verbose=10, n_jobs=-1)\n",
    "cv.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = df_train_sta.drop([\"idx\",\"y\"], axis=1)\n",
    "y = df_train_sta.y\n",
    "train_X, test_X, train_y, test_y = train_test_split(x, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "cv = GridSearchCV(classifier, parameters, cv=4, scoring=\"roc_auc\", verbose=10, n_jobs=-1)\n",
    "cv.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pred = calc_proba(cv.predict_proba(test_X))\n",
    "print(\"best model auc:\", roc_auc_score(test_y, pred))\n",
    "cv.best_estimator_.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "#DeepLearning（適当）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "y_cate = keras.utils.np_utils.to_categorical(y,  2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=x.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#学習の実行\n",
    "epochs=128\n",
    "batch_size=100\n",
    "history1 = model.fit(x, y_cate,  epochs=epochs, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
