{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os, re, gc, sys, time, random, warnings, functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from datetime import datetime\n",
    "from pykalman import KalmanFilter\n",
    "from IPython.display import display\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, TimeSeriesSplit, GroupKFold, GroupShuffleSplit\n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "!pip install tensorflow_addons==0.9.1\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "def reduce_mem_usage(df, verbose=True, y=['time','open_channels']):\n",
    "    numerics  = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col in y or col_type not in numerics:\n",
    "            continue\n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            if   c_min > np.iinfo(np.int8).min  and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)  \n",
    "        else:\n",
    "            if   c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                df[col] = df[col].astype(np.float16)\n",
    "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def show_cols(show_list, show_num=50, col=True):\n",
    "    reshaped_list = []\n",
    "    if show_num < len(show_list):\n",
    "        for i in range(0, len(show_list)+show_num, show_num):\n",
    "            if len(show_list) < i: break\n",
    "            l = sorted(show_list)[i:i+show_num]\n",
    "            if len(l)==show_num:\n",
    "                reshaped_list.append(l)\n",
    "            else:\n",
    "                reshaped_list.append(l + [None]*(show_num-len(l)))\n",
    "    else:\n",
    "        reshaped_list = [sorted(show_list)]\n",
    "    df_show_col = pd.DataFrame(reshaped_list)\n",
    "    if 0 < df_show_col.shape[1]:\n",
    "        display(df_show_col) if col else display(df_show_col.T)\n",
    "    else:\n",
    "        print(\"No features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kalman1D(observations, damping=1):\n",
    "    # To return the smoothed time series data\n",
    "    observation_covariance = damping\n",
    "    initial_value_guess    = observations[0]\n",
    "    transition_matrix      = 1\n",
    "    transition_covariance  = 0.1\n",
    "    kf = KalmanFilter(\n",
    "            initial_state_mean       = initial_value_guess,\n",
    "            initial_state_covariance = observation_covariance,\n",
    "            observation_covariance   = observation_covariance,\n",
    "            transition_covariance    = transition_covariance,\n",
    "            transition_matrices      = transition_matrix)\n",
    "    pred_state, state_cov = kf.smooth(observations)\n",
    "    del kf\n",
    "    gc.collect()\n",
    "    return pred_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(train, test):\n",
    "    # Concatenate data\n",
    "    batch_size=500000; batch2_size=4000\n",
    "    train['set'] = 'train'\n",
    "    test ['set'] = 'test'\n",
    "    data  = pd.concat([train, test], sort=False)\n",
    "    # Add batch and batch2\n",
    "    data['batch']  = (data.groupby(data.index//batch_size,  sort=False)['signal'].agg(['ngroup']).values).astype(int)\n",
    "    data['batch2'] = (data.groupby(data.index//batch2_size, sort=False)['signal'].agg(['ngroup']).values).astype(int)\n",
    "    train = data[data['set']=='train'].drop('set', axis=1).copy()\n",
    "    test  = data[data['set']=='test'] .drop('set', axis=1).copy()\n",
    "    del data\n",
    "    return train, test\n",
    "\n",
    "def normalize(train, test):\n",
    "    train_mean  = np.mean(train.signal.tolist())\n",
    "    train_sigma = np.std(train.signal.tolist())\n",
    "    train['signal'] = (train.signal - train_mean) / train_sigma\n",
    "    test['signal']  = (test.signal  - train_mean) / train_sigma\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal processing features\n",
    "def calc_gradients(s, n_grads = 4):\n",
    "    '''\n",
    "    Calculate gradients for a pandas series. Returns the same number of samples\n",
    "    '''\n",
    "    grads = pd.DataFrame()    \n",
    "    g = s.values\n",
    "    for i in range(n_grads):\n",
    "        g = np.gradient(g)\n",
    "        grads['grad_' + str(i+1)] = g\n",
    "    return grads\n",
    "\n",
    "def calc_low_pass(s, n_filts=10):\n",
    "    '''\n",
    "    Applies low pass filters to the signal. Left delayed and no delayed\n",
    "    '''\n",
    "    wns = np.logspace(-2, -0.3, n_filts)\n",
    "    low_pass = pd.DataFrame()\n",
    "    x = s.values\n",
    "    for wn in wns:\n",
    "        b, a = signal.butter(1, Wn=wn, btype='low')\n",
    "        zi = signal.lfilter_zi(b, a)\n",
    "        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n",
    "        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)        \n",
    "    return low_pass\n",
    "\n",
    "def calc_high_pass(s, n_filts=10):\n",
    "    '''\n",
    "    Applies high pass filters to the signal. Left delayed and no delayed\n",
    "    '''\n",
    "    wns = np.logspace(-2, -0.1, n_filts)\n",
    "    high_pass = pd.DataFrame()\n",
    "    x = s.values\n",
    "    for wn in wns:\n",
    "        b, a = signal.butter(1, Wn=wn, btype='high')\n",
    "        zi = signal.lfilter_zi(b, a)\n",
    "        high_pass['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n",
    "        high_pass['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)        \n",
    "    return high_pass\n",
    "\n",
    "def calc_ewm(s, windows=[10, 50, 100, 500, 1000]):\n",
    "    '''\n",
    "    Calculates exponential weighted functions\n",
    "    '''\n",
    "    ewm = pd.DataFrame()\n",
    "    for w in windows:\n",
    "        ewm['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n",
    "        ewm['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n",
    "    # add zeros when na values (std)\n",
    "    ewm = ewm.fillna(value=0)\n",
    "    return ewm\n",
    "\n",
    "def add_features(s):\n",
    "    '''\n",
    "    All calculations together\n",
    "    '''\n",
    "    gradients = calc_gradients(s)\n",
    "    low_pass  = calc_low_pass(s)\n",
    "    high_pass = calc_high_pass(s)\n",
    "    ewm       = calc_ewm(s)    \n",
    "    return pd.concat([s, gradients, low_pass, high_pass, ewm], axis=1)\n",
    "\n",
    "def divide_and_add_features(s, signal_size=500000):\n",
    "    '''\n",
    "    Divide the signal in bags of \"signal_size\".\n",
    "    Normalize the data dividing it by 15.0\n",
    "    '''\n",
    "    # normalize\n",
    "    s = s / 15.0\n",
    "    \n",
    "    ls = []\n",
    "    for i in range(int(s.shape[0]/signal_size)):\n",
    "        sig = s[i*signal_size:(i+1)*signal_size].copy().reset_index(drop=True)\n",
    "        sig_featured = add_features(sig)\n",
    "        ls.append(sig_featured)\n",
    "        \n",
    "    return pd.concat(ls, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling and aggreagate batch features\n",
    "def rolling_features(df, periods, windows, g=\"batch\", c=\"signal\"):\n",
    "    df = df.copy()\n",
    "    periods_lead = np.asarray(periods, dtype=np.int32)\n",
    "    periods_lag  = -periods_lead\n",
    "    for p in periods_lead:\n",
    "        df[c+\"_lead_t\"+str(p)]    = df.groupby(g)[c].shift(periods=p)\n",
    "    for p in periods_lag:\n",
    "        df[c+\"_lag_t\" +str(-1*p)] = df.groupby(g)[c].shift(periods=p)\n",
    "\n",
    "    for window in windows:\n",
    "        # roll backwards\n",
    "        df[c+'_mean_t'+str(window)] = df.groupby(g)[c].transform(lambda x: x.shift(1).rolling(window).mean())\n",
    "        df[c+'_std_t' +str(window)] = df.groupby(g)[c].transform(lambda x: x.shift(1).rolling(window).std())\n",
    "        df[c+'_var_t' +str(window)] = df.groupby(g)[c].transform(lambda x: x.shift(1).rolling(window).var())\n",
    "        df[c+'_min_t' +str(window)] = df.groupby(g)[c].transform(lambda x: x.shift(1).rolling(window).min())\n",
    "        df[c+'_max_t' +str(window)] = df.groupby(g)[c].transform(lambda x: x.shift(1).rolling(window).max())\n",
    "        min_max = (df[c] - df[c+'_min_t'+str(window)]) / (df[c+'_max_t'+str(window)] - df[c+'_min_t'+str(window)])\n",
    "        df[c+'_norm_t'+str(window)] = min_max * (np.floor(df[c+'_max_t'+str(window)]) - np.ceil(df[c+'_min_t'+str(window)]))\n",
    "        # roll forward\n",
    "        df[c+'_mean_t'+str(window)+'_lead'] = df.groupby(g)[c].transform(lambda x: x.shift(-window-1).rolling(window).mean())\n",
    "        df[c+'_std_t' +str(window)+'_lead'] = df.groupby(g)[c].transform(lambda x: x.shift(-window-1).rolling(window).std())\n",
    "        df[c+'_var_t' +str(window)+'_lead'] = df.groupby(g)[c].transform(lambda x: x.shift(-window-1).rolling(window).var())\n",
    "        df[c+'_min_t' +str(window)+'_lead'] = df.groupby(g)[c].transform(lambda x: x.shift(-window-1).rolling(window).min())\n",
    "        df[c+'_max_t' +str(window)+'_lead'] = df.groupby(g)[c].transform(lambda x: x.shift(-window-1).rolling(window).max())\n",
    "        min_max = (df[c] - df[c+'_min_t'+str(window)+'_lead']) / (df[c+'_max_t'+str(window)+'_lead'] - df[c+'_min_t'+str(window)+'_lead'])\n",
    "        df[c+'_norm_t'+str(window)+'_lead'] = min_max * (np.floor(df[c+'_max_t'+str(window)+'_lead']) - np.ceil(df[c+'_min_t'+str(window)+'_lead']))\n",
    "    return df\n",
    "\n",
    "def static_batch_features(df, n, c=\"signal\", add_detail_feats=True):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(by='time').reset_index(drop=True)\n",
    "    df.index = ((df.time * 10000) - 1).values\n",
    "    df['batch_'        +str(n)] = df.index // n\n",
    "    df['batch_index_'  +str(n)] = df.index - (df['batch_'+str(n)] * n)\n",
    "    df['batch_slices_' +str(n)] = df['batch_index_'+str(n)] // (n / 10)\n",
    "    df['batch_slices2_'+str(n)] = df['batch_'+str(n)].astype(str).str.zfill(3) + \"_\" + df['batch_slices_'+str(n)].astype(str).str.zfill(3)\n",
    "    \n",
    "    for g in ['batch_'+str(n), 'batch_slices2_'+str(n)]:\n",
    "        d = {}\n",
    "        # -----------------------------------------------\n",
    "        d[c+'_mean'  +g]   = df.groupby(g)[c].mean()\n",
    "        d[c+'_median'+g]   = df.groupby(g)[c].median()\n",
    "        d[c+'_std'   +g]   = df.groupby(g)[c].std()\n",
    "        if add_detail_feats:\n",
    "            d[c+'_min'   +g]   = df.groupby(g)[c].min()\n",
    "            d[c+'_max'   +g]   = df.groupby(g)[c].max()\n",
    "            min_max            = (d[c+'_mean'+g]-d[c+'_min'+g]) / (d[c+'_max'+g]-d[c+'_min'+g])\n",
    "            d[c+'_norm'    +g] = min_max * (np.floor(d[c+'_max'+g])-np.ceil(d[c+'_min'+g]))\n",
    "            d[c+'_p5'    +g]   = df.groupby(g)[c].apply(lambda x: np.percentile(x, 5))\n",
    "            d[c+'_p10'   +g]   = df.groupby(g)[c].apply(lambda x: np.percentile(x, 10))\n",
    "            d[c+'_p25'   +g]   = df.groupby(g)[c].apply(lambda x: np.percentile(x, 25))\n",
    "            d[c+'_p75'   +g]   = df.groupby(g)[c].apply(lambda x: np.percentile(x, 75))\n",
    "            d[c+'_p90'   +g]   = df.groupby(g)[c].apply(lambda x: np.percentile(x, 90))\n",
    "            d[c+'_p95'   +g]   = df.groupby(g)[c].apply(lambda x: np.percentile(x, 95))\n",
    "            d[c+'_skew'  +g]   = df.groupby(g)[c].apply(lambda x: pd.Series(x).skew())\n",
    "            d[c+'_kurtosis'+g] = df.groupby(g)[c].apply(lambda x: pd.Series(x).kurtosis())\n",
    "            d[c+'_mean_abs_chg'+g] = df.groupby(g)[c].apply(lambda x: np.mean(np.abs(np.diff(x))))\n",
    "            d[c+'_abs_max' +g] = df.groupby(g)[c].apply(lambda x: np.max(np.abs(x)))\n",
    "            d[c+'_abs_min' +g] = df.groupby(g)[c].apply(lambda x: np.min(np.abs(x)))\n",
    "            d[c+'_range'   +g] = d[c+'_max'+g] - d[c+'_min'+g]\n",
    "            d[c+'_maxtomin'+g] = d[c+'_max'+g] / d[c+'_min'+g]\n",
    "            d[c+'_abs_avg' +g] = (d[c+'_abs_min'+g] + d[c+'_abs_max'+g]) / 2\n",
    "        # -----------------------------------------------\n",
    "        for v in d:\n",
    "            df[v] = df[g].map(d[v].to_dict())\n",
    "\n",
    "    drop_cols  = [\"time\",\"open_channels\",\"signal\",\"signal_power\"] + [col for col in df.columns if col.startswith(\"batch\")]\n",
    "    drop_cols  = [col for col in drop_cols if col != c]\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "    for diffc in df.columns:\n",
    "        if c == diffc: continue\n",
    "        df[diffc+'_m'+c] = df[diffc] - df[c]\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result_lgbm_regression(train_test, df=None, add_static_feats=False, n=25000, save=True):\n",
    "    df_reg_preds = pd.read_csv(\"../input/results-for-ionswitching/lgbm_regression_%s_preds.csv\" % train_test,\n",
    "                               names=[\"lgbm_reg_result\"], skiprows=1)\n",
    "    if save:\n",
    "        df_reg_preds.to_csv(\"./lgbm_regression_%s_preds.csv\" % train_test, index=False)\n",
    "    if add_static_feats and df is not None:\n",
    "        df_reg_preds = pd.concat([df, df_reg_preds], axis=1)\n",
    "        df_reg_preds = static_batch_features(df_reg_preds, n, \"lgbm_reg_result\", False)\n",
    "    return df_reg_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main parameters\n",
    "CALCULATE_KALMAN = True   # If false, calculated kalman data will be loaded.\n",
    "OUTPUT_FEATURES  = True   # If true, feature importance file will be generated.\n",
    "ENSEMBLE_MODELS  = False  # If true, LGBM result and WaveNet result will be ensembled.\n",
    "TRAIN_WAVENET    = False\n",
    "TRAIN_LGBM       = True\n",
    "LGBM_REGRESSION  = True   # If true, LGBM will train on regression task. If not so, multiclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('../input/data-without-drift/train_clean.csv')\n",
    "test  = pd.read_csv('../input/data-without-drift/test_clean.csv')\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test  = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if CALCULATE_KALMAN:\n",
    "    # Kalman Filter\n",
    "    observation_covariance = .0015\n",
    "    train.signal = Kalman1D(train.signal.values, observation_covariance)\n",
    "    test.signal  = Kalman1D(test.signal.values,  observation_covariance)\n",
    "else:\n",
    "    train = pd.read_csv(\"../input/results-for-ionswitching/train_kalman.csv\")\n",
    "    test  = pd.read_csv(\"../input/results-for-ionswitching/test_kalman.csv\")   \n",
    "    train = reduce_mem_usage(train)\n",
    "    test  = reduce_mem_usage(test)\n",
    "    \n",
    "# Save created data for next kernel.\n",
    "train.time = train.time.apply(lambda x: round(x,4))\n",
    "test. time = test .time.apply(lambda x: round(x,4))\n",
    "train.to_csv('./train_kalman.csv', index=False)\n",
    "test .to_csv('./test_kalman.csv',  index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Replace some outliers.\n",
    "train.loc[478587:478588, \"signal\"] = -2\n",
    "train.loc[478609:478610, \"signal\"] = -2\n",
    "\n",
    "# Normalize signal and add batches.\n",
    "# train, test = normalize(train, test)\n",
    "train, test = get_batch(train, test)\n",
    "\n",
    "# Add signal power\n",
    "train[\"signal_power\"] = train.signal ** 2\n",
    "test [\"signal_power\"] = test .signal ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Except outlier\n",
    "non_outlier_batch7 = pd.concat([train[3500000:3642922], train[3822754:4000000]], sort=False)\n",
    "train = pd.concat([train[:3500000], non_outlier_batch7, train[4000000:]]).reset_index(drop=True)\n",
    "train = train.sort_values(by=\"time\")\n",
    "\n",
    "print(train.shape)\n",
    "del non_outlier_batch7\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_importance_features(thr_no_imp):\n",
    "    path = \"../input/results-for-ionswitching/features_of_lgbm_regression.csv\"\n",
    "    if not os.path.isfile(path):\n",
    "        print(\"'features_of_lgbm_regression.csv' is not exist.\")\n",
    "        print(\"Since this is the first kernel, features file has not been generated yet.\")\n",
    "        return []\n",
    "    df_imp     = pd.read_csv(path)\n",
    "    df_imp.to_csv(\"./features_of_lgbm_regression.csv\", index=False)\n",
    "    feats_len  = df_imp.features.nunique()\n",
    "    no_feats   = list(df_imp.query(\"importance_mean <= @thr_no_imp\").features.unique())\n",
    "    print(\"Ratio of features will be used is %s%%.\" % (round(((feats_len-len(no_feats))/feats_len*100),2)))\n",
    "    print(\"The number of deleted features is %s(all %s).\" % (len(no_feats), feats_len))\n",
    "    show_cols(no_feats)\n",
    "    return no_feats\n",
    "\n",
    "def delete_features(df, no_feats):\n",
    "    if len(no_feats)==0:\n",
    "        return df\n",
    "    drop_no_feats = [c for c in df.columns if c in no_feats]\n",
    "    print(\"Ratio of deleted featrues is %s(%s/%s)\"\n",
    "          % (len(drop_no_feats)/len(df.columns), len(drop_no_feats), len(df.columns)))\n",
    "    return df.drop(drop_no_feats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "if TRAIN_LGBM:\n",
    "    cols_rolling = [\"signal\",\"signal_power\"]\n",
    "    cols_static  = {\"25000\" : [\"signal\",\"signal_power\"],\n",
    "                    \"50000\" : []}\n",
    "    group   = \"batch\"\n",
    "    periods = range(1,5)\n",
    "    windows = [10,50,100,1000]\n",
    "    add_static_feats = [True, 25000]\n",
    "    thr_no_imp       = -1\n",
    "else:\n",
    "    cols_rolling = [\"signal\"]\n",
    "    cols_static  = {\"4000\" : [\"signal\"]}\n",
    "    group   = \"batch2\"\n",
    "    periods = range(1,2)\n",
    "    windows = [100]\n",
    "    add_static_feats = [False, 4000]\n",
    "    thr_no_imp       = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_feats = get_no_importance_features(thr_no_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create features for training\n",
    "dfs_train = [train]\n",
    "\n",
    "print(\"Processing for 'rolling_features'\")\n",
    "dcols = [\"signal\",\"signal_power\",\"time\",\"open_channels\",\"batch\",\"batch2\"]\n",
    "for col in cols_rolling:\n",
    "    df = reduce_mem_usage(rolling_features(train, periods, windows, g=group, c=col).drop(dcols, axis=1))\n",
    "    dfs_train.append(delete_features(df, no_feats))\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "if TRAIN_WAVENET or (not LGBM_REGRESSION):\n",
    "    print(\"=================================\")\n",
    "    print(\"Processing for 'load_result_lgbm_regression'\")\n",
    "    add, n = add_static_feats\n",
    "    dfs_train.append(reduce_mem_usage(load_result_lgbm_regression(\"train\", train, add, n)))\n",
    "        \n",
    "if TRAIN_LGBM:\n",
    "    print(\"=================================\")\n",
    "    print(\"Processing for 'divide_and_add_features'\")\n",
    "    df = reduce_mem_usage(divide_and_add_features(train.signal).drop(\"signal\", axis=1))\n",
    "    dfs_train.append(delete_features(df, no_feats))\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"=================================\")\n",
    "    print(\"Processing for 'static_batch_features'\")\n",
    "    for n, cols in cols_static.items():\n",
    "        for col in cols:\n",
    "            df = reduce_mem_usage(static_batch_features(train, int(n), col).drop(col, axis=1))\n",
    "            dfs_train.append(delete_features(df, no_feats))\n",
    "        \n",
    "del df, train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join features for training.\n",
    "train = pd.concat(dfs_train, axis=1, sort=False)\n",
    "print(train.shape)\n",
    "del dfs_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create features for testing.\n",
    "dfs_test = [test]\n",
    "\n",
    "print(\"Processing for 'rolling_features'\")\n",
    "dcols = [\"signal\",\"signal_power\",\"time\",\"open_channels\",\"batch\",\"batch2\"]\n",
    "for col in cols_rolling:\n",
    "    df = reduce_mem_usage(rolling_features(test, periods, windows, g=group, c=col).drop(dcols, axis=1))\n",
    "    dfs_test.append(delete_features(df, no_feats))\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "if TRAIN_WAVENET or (not LGBM_REGRESSION):\n",
    "    print(\"=================================\")\n",
    "    print(\"Processing for 'load_result_lgbm_regression'\")\n",
    "    add, n = add_static_feats\n",
    "    dfs_test.append(reduce_mem_usage(load_result_lgbm_regression(\"test\", test, add, n)))\n",
    "\n",
    "if TRAIN_LGBM:\n",
    "    print(\"=================================\")\n",
    "    print(\"Processing for 'divide_and_add_features'\")\n",
    "    df = reduce_mem_usage(divide_and_add_features(test.signal).drop(\"signal\", axis=1))\n",
    "    dfs_test.append(delete_features(df, no_feats))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"=================================\")\n",
    "    print(\"Processing for 'static_batch_features'\")\n",
    "    for n, cols in cols_static.items():\n",
    "        for col in cols:\n",
    "            df = reduce_mem_usage(static_batch_features(test, int(n), col).drop(col, axis=1))\n",
    "            dfs_test.append(delete_features(df, no_feats))\n",
    "    \n",
    "del df, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join features for testing.\n",
    "test = pd.concat(dfs_test, axis=1, sort=False)\n",
    "print(test.shape)\n",
    "del dfs_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if TRAIN_WAVENET:\n",
    "    print(\"Processing train data...\")\n",
    "    for feat in train.columns:\n",
    "        if train[train[feat].isnull()].shape[0]==0: continue\n",
    "        for batch in train.batch.unique():\n",
    "            _mean = np.mean(train[(train.batch==batch)&(train[feat].notnull())][feat].tolist())\n",
    "            train.loc[(train.batch==batch)&(train[feat].isnull()), feat] = _mean\n",
    "            \n",
    "    print(\"Processing test data...\")\n",
    "    for feat in test.columns:\n",
    "        if test[test[feat].isnull()].shape[0]==0: continue\n",
    "        for batch in test.batch.unique():\n",
    "            _mean = np.mean(test[(test.batch==batch)&(test[feat].notnull())][feat].tolist())\n",
    "            test.loc[(test.batch==batch)&(test[feat].isnull()), feat] = _mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "display(train.head())\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    \"\"\"\n",
    "    Base Model Class:\n",
    "\n",
    "    train_df         : train pandas dataframe\n",
    "    test_df          : test pandas dataframe\n",
    "    target           : target column name (str)\n",
    "    features         : list of feature names\n",
    "    categoricals     : list of categorical feature names\n",
    "    n_splits         : K in KFold (default is 3)\n",
    "    cv_method        : options are .. KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold, or GroupShuffleSplit\n",
    "    group            : group feature name when GroupKFold or StratifiedGroupKFold are used\n",
    "    task             : options are .. regression, multiclass, or binary\n",
    "    param            : dict of parameter, set that if you already define\n",
    "    parameter_tuning : bool, only for LGB\n",
    "    seed             : seed (int)\n",
    "    verbose          : bool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_df, test_df, target, features, \n",
    "                 valid_df=None, categoricals=[], \n",
    "                 n_splits=3, cv_method=\"KFold\", group=None,\n",
    "                 task=\"regression\", params=None, parameter_tuning=False,\n",
    "                 seed=42, verbose=True):\n",
    "        self.train_df     = train_df\n",
    "        if valid_df is not None and valid_df.shape[0]==0:\n",
    "            self.valid_df = None            \n",
    "        else:\n",
    "            self.valid_df = valid_df\n",
    "        self.test_df      = test_df\n",
    "        self.target       = target\n",
    "        self.features     = features\n",
    "        self.n_splits     = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.cv_method    = cv_method\n",
    "        self.group        = group\n",
    "        self.task         = task\n",
    "        self.parameter_tuning = parameter_tuning\n",
    "        self.seed    = seed\n",
    "        self.cv      = self.get_cv()\n",
    "        self.verbose = verbose\n",
    "        if params is None:\n",
    "            self.params = self.get_params()\n",
    "        else:\n",
    "            self.params = params\n",
    "        self.y_pred, self.y_valid, self.score, self.model, self.oof, self.y_val, self.fi_df = self.fit()\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def calc_metric(self, y_true, y_pred): # this may need to be changed based on the metric of interest\n",
    "        if self.task in (\"multiclass\",\"nn_multiclass\"):\n",
    "            preds = np.argmax(y_pred, axis=1) if y_true.shape != y_pred.shape else y_pred\n",
    "            return f1_score(y_true, preds, average='macro')                \n",
    "        if self.task == \"binary\":\n",
    "            return f1_score(y_true, y_pred, average='macro')\n",
    "        if self.task == \"regression\":\n",
    "            preds = np.round(np.clip(y_pred, 0, 10)).astype(int)\n",
    "            return f1_score(y_true, preds, average='macro')\n",
    "\n",
    "    def get_cv(self):\n",
    "        if self.cv_method == \"KFold\":\n",
    "            cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n",
    "            return cv.split(self.train_df)\n",
    "        if self.cv_method == \"StratifiedKFold\":\n",
    "            cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n",
    "            return cv.split(self.train_df, self.train_df[self.target])\n",
    "        if self.cv_method == \"TimeSeriesSplit\":\n",
    "            cv = TimeSeriesSplit(max_train_size=None, n_splits=self.n_splits)\n",
    "            return cv.split(self.train_df)\n",
    "        if self.cv_method == \"GroupKFold\":\n",
    "            if self.group in self.features:\n",
    "                self.features.remove(self.group)\n",
    "            cv = GroupKFold(n_splits=self.n_splits)\n",
    "            return cv.split(self.train_df[self.features], self.train_df[self.target], self.train_df[self.group])\n",
    "        if self.cv_method == \"GroupShuffleSplit\":\n",
    "            if self.group in self.features:\n",
    "                self.features.remove(self.group)\n",
    "            cv = GroupShuffleSplit(n_splits=self.n_splits, random_state=self.seed)\n",
    "            return cv.split(self.train_df[self.features], self.train_df[self.target], self.train_df[self.group])\n",
    "\n",
    "    def fit(self):\n",
    "        # Initialize\n",
    "        y_vals = np.zeros((self.train_df.shape[0], ))\n",
    "        if self.task in (\"multiclass\",\"nn_multiclass\"):\n",
    "            oof_pred = np.zeros((self.train_df.shape[0], self.train_df[self.target].nunique()))\n",
    "            y_pred   = np.zeros((self.test_df.shape[0],  self.train_df[self.target].nunique()))\n",
    "            if self.valid_df is not None:\n",
    "                y_valid = np.zeros((self.valid_df.shape[0], self.train_df[self.target].nunique()))\n",
    "            else:\n",
    "                y_valid = None\n",
    "        else:\n",
    "            oof_pred = np.zeros((self.train_df.shape[0], ))\n",
    "            y_pred   = np.zeros((self.test_df.shape[0], ))\n",
    "            if self.valid_df is not None:\n",
    "                y_valid = np.zeros((self.valid_df.shape[0], ))\n",
    "            else:\n",
    "                y_valid = None\n",
    "            \n",
    "        if self.group is not None:\n",
    "            if self.group in self.features:\n",
    "                self.features.remove(self.group)\n",
    "            if self.group in self.categoricals:\n",
    "                self.categoricals.remove(self.group)\n",
    "                \n",
    "        fi = np.zeros((self.n_splits, len(self.features)))\n",
    "        if y_valid is not None:\n",
    "            x_valid = self.valid_df[self.features].copy()\n",
    "            del self.valid_df\n",
    "            gc.collect()\n",
    "        x_test = self.test_df[self.features]\n",
    "\n",
    "        # Fitting with out of fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            # Prepare train and test dataset\n",
    "            x_train, x_val     = self.train_df.loc[train_idx, self.features], self.train_df.loc[val_idx, self.features]\n",
    "            y_train, y_val     = self.train_df.loc[train_idx, self.target],   self.train_df.loc[val_idx, self.target]\n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            del x_train, y_train\n",
    "            gc.collect()\n",
    "            \n",
    "            # Fit model\n",
    "            model, importance = self.train_model(train_set, val_set)\n",
    "            fi[fold, :]       = importance\n",
    "            y_vals[val_idx]   = y_val\n",
    "            \n",
    "            # Get some scores\n",
    "            if   self.task == \"binary\":\n",
    "                oof_pred[val_idx] = model.predict(x_val).reshape(oof_pred[val_idx].shape)\n",
    "                y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "                \n",
    "            elif self.task == \"regression\":\n",
    "                oof_pred[val_idx] = model.predict(x_val).reshape(oof_pred[val_idx].shape)\n",
    "                if y_valid is not None:\n",
    "                    y_valid += model.predict(x_valid).reshape(y_valid.shape) / self.n_splits\n",
    "                y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "                \n",
    "            elif self.task == \"multiclass\":\n",
    "                oof_pred[val_idx] = model.predict(x_val)\n",
    "                if y_valid is not None:\n",
    "                    y_valid += model.predict(x_valid).reshape(y_valid.shape) / self.n_splits\n",
    "                y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "                \n",
    "            elif self.task == \"nn_multiclass\":\n",
    "                preds   = model.predict(val_set[0])\n",
    "                oof_pred[val_idx] = preds.reshape(-1, preds.shape[-1])\n",
    "                if y_valid is not None:\n",
    "                    preds    = model.predict(self.convert_dataset(x_valid))\n",
    "                    y_valid += preds.reshape(-1, preds.shape[-1]) / self.n_splits\n",
    "                preds   = model.predict(self.convert_dataset(x_test))\n",
    "                y_pred += preds.reshape(-1, preds.shape[-1]) / self.n_splits\n",
    "                \n",
    "            print('Partial score of fold {} is: {}'.format(fold, self.calc_metric(y_val, oof_pred[val_idx])))\n",
    "        \n",
    "        # Create feature importance data frame\n",
    "        fi_df = pd.DataFrame()\n",
    "        for n in np.arange(self.n_splits):\n",
    "            tmp = pd.DataFrame()\n",
    "            tmp[\"features\"]   = self.features\n",
    "            tmp[\"importance\"] = fi[n, :]\n",
    "            tmp[\"fold\"]       = n\n",
    "            fi_df = pd.concat([fi_df, tmp], ignore_index=True)\n",
    "        gfi   = fi_df[[\"features\", \"importance\"]].groupby([\"features\"]).mean().reset_index()\n",
    "        fi_df = fi_df.merge(gfi, on=\"features\", how=\"left\", suffixes=('', '_mean'))\n",
    "        \n",
    "        # Calculate oof score\n",
    "        loss_score = self.calc_metric(y_vals, oof_pred)\n",
    "        print('Our oof loss score is: ', loss_score)\n",
    "        \n",
    "        return y_pred, y_valid, loss_score, model, oof_pred, y_vals, fi_df\n",
    "\n",
    "    def plot_feature_importance(self, rank_range=[1, 50]):\n",
    "        fig, ax   = plt.subplots(1, 1, figsize=(10, 20))\n",
    "        sorted_df = self.fi_df.sort_values(by=\"importance_mean\", ascending=False).reset_index()\n",
    "        sns.barplot(data=sorted_df.iloc[self.n_splits*(rank_range[0]-1) : self.n_splits*rank_range[1]],\n",
    "                    x=\"importance\", y=\"features\", orient='h')\n",
    "        ax.set_xlabel(\"feature importance\")\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LgbModel(BaseModel):\n",
    "    \"\"\"\n",
    "    LGB wrapper\n",
    "    \"\"\"\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        model     = lgb.train(self.params, train_set, num_boost_round=4000, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n",
    "        fi        = model.feature_importance(importance_type=\"gain\")\n",
    "        return model, fi\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val=None, y_val=None):\n",
    "        train_set   = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n",
    "        if x_val is not None:\n",
    "            val_set = lgb.Dataset(x_val,   y_val,   categorical_feature=self.categoricals)\n",
    "            return train_set, val_set\n",
    "        return train_set\n",
    "\n",
    "    def get_params(self):\n",
    "        # Fast fit parameters\n",
    "        params = {\n",
    "          'boosting_type'    : \"gbdt\",\n",
    "          'objective'        : self.task,\n",
    "          'num_leaves'       : 127,\n",
    "          'max_depth'        : -1,\n",
    "          'min_data_in_leaf' : 50,\n",
    "          'learning_rate'    : 0.005,\n",
    "          'early_stopping_rounds' : 50,\n",
    "          'bagging_seed'     : 11,\n",
    "          'random_state'     : 42,\n",
    "          'verbosity'        : -1\n",
    "         }\n",
    "\n",
    "        # List is here: https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "        if self.task == \"regression\":\n",
    "            params[\"metric\"] = \"rmse\"\n",
    "        if self.task == \"binary\":\n",
    "            params[\"metric\"] = \"binary_logloss\"\n",
    "        if self.task == \"multiclass\":\n",
    "            params[\"metric\"]    = \"multi_logloss\"\n",
    "            params[\"num_class\"] = len(self.train_df[self.target].unique())\n",
    "        \n",
    "        # Bayesian Optimization by Optuna\n",
    "        if self.parameter_tuning:\n",
    "            # Define objective function\n",
    "            def objective(trial):\n",
    "                # Split train and test data\n",
    "                train_x, test_x, train_y, test_y = train_test_split(self.train_df[self.features], \n",
    "                                                                    self.train_df[self.target],\n",
    "                                                                    test_size=0.3, random_state=self.seed)\n",
    "                \n",
    "                dtrain = lgb.Dataset(train_x, train_y, categorical_feature=self.categoricals)\n",
    "                dtest  = lgb.Dataset(test_x,  test_y,  categorical_feature=self.categoricals)\n",
    "\n",
    "                # Parameters to be explored\n",
    "                hyperparams = {'num_leaves'        : trial.suggest_int('num_leaves', 24, 1024),\n",
    "                               'max_depth'         : trial.suggest_int('max_depth', 4, 16),\n",
    "                               'min_child_weight'  : trial.suggest_int('min_child_weight', 1, 20),\n",
    "                               'feature_fraction'  : trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "                               'bagging_fraction'  : trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "                               'bagging_freq'      : trial.suggest_int('bagging_freq', 1, 7),\n",
    "                               'min_child_samples' : trial.suggest_int('min_child_samples', 5, 100),\n",
    "                               'lambda_l1'         : trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "                               'lambda_l2'         : trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "                               'early_stopping_rounds' : 100}\n",
    "                # LGBM\n",
    "                params.update(hyperparams)\n",
    "                model = lgb.train(params, dtrain, valid_sets=dtest, verbose_eval=500)\n",
    "                pred  = model.predict(test_x)\n",
    "                if self.task == \"regression\":\n",
    "                    pred = np.round(np.clip(pred, 0, 10)).astype(int)\n",
    "                return self.calc_metric(test_y, pred)\n",
    "\n",
    "            # Run optimization\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=50)\n",
    "            print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "            print('Best trial:')\n",
    "            trial = study.best_trial\n",
    "            print('  Value: {}'.format(trial.value))\n",
    "            print('  Params: ')\n",
    "            for key, value in trial.params.items():\n",
    "                print('    {}: {}'.format(key, value))\n",
    "\n",
    "            params.update(trial.params)\n",
    "            params[\"learning_rate\"] = 0.001\n",
    "            # Plot history\n",
    "            plot_optimization_history(study)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(BaseModel):\n",
    "    \"\"\"\n",
    "    Wave Net wrapper\n",
    "    \"\"\"    \n",
    "    def get_params(self):\n",
    "        params = {\n",
    "            \"batch_size\"    : 4000,\n",
    "            \"num_classes\"   : len(self.train_df[self.target].unique()),\n",
    "            \"learning_rate\" : 0.0015,\n",
    "            \"nn_epochs\"     : 180,\n",
    "            \"nn_batch_size\" : 16,\n",
    "            \"patience\"      : 25\n",
    "        }\n",
    "        display(params)\n",
    "        return params\n",
    "\n",
    "    def get_model(self, shape_):\n",
    "        \n",
    "        def cbr(x, out_layer, kernel, stride, dilation):\n",
    "            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation(\"relu\")(x)\n",
    "            return x\n",
    "    \n",
    "        def wave_block(x, filters, kernel_size, n):\n",
    "            dilation_rates = [2**i for i in range(n)]\n",
    "            x     = Conv1D(filters=filters, kernel_size=1, padding='same')(x)\n",
    "            res_x = x\n",
    "            for dilation_rate in dilation_rates:\n",
    "                tanh_out = Conv1D(filters=filters, kernel_size=kernel_size, padding='same', activation='tanh',    dilation_rate=dilation_rate)(x)\n",
    "                sigm_out = Conv1D(filters=filters, kernel_size=kernel_size, padding='same', activation='sigmoid', dilation_rate=dilation_rate)(x)\n",
    "                x     = Multiply()([tanh_out, sigm_out])\n",
    "                x     = Conv1D(filters=filters, kernel_size=1, padding='same')(x)\n",
    "                res_x = Add()([res_x, x])\n",
    "            return res_x\n",
    "    \n",
    "        inp = Input(shape=(shape_))\n",
    "        x   = cbr(inp, 64, 7, 1, 1)\n",
    "        x   = BatchNormalization()(x)\n",
    "        x   = wave_block(x, 16, 3, 12)\n",
    "        x   = BatchNormalization()(x)\n",
    "        x   = wave_block(x, 32, 3, 8)\n",
    "        x   = BatchNormalization()(x)\n",
    "        x   = wave_block(x, 64, 3, 4)\n",
    "        x   = BatchNormalization()(x)\n",
    "        x   = wave_block(x, 128, 3, 1)\n",
    "        x   = cbr(x, 32, 7, 1, 1)\n",
    "        x   = BatchNormalization()(x)\n",
    "        x   = wave_block(x, 64, 3, 1)\n",
    "        x   = cbr(x, 32, 7, 1, 1)\n",
    "        x   = BatchNormalization()(x)\n",
    "        x   = Dropout(0.2)(x)\n",
    "        out = Dense(11, activation='softmax', name='out')(x)\n",
    "\n",
    "        model = models.Model(inputs=inp, outputs=out)\n",
    "        opt   = Adam(lr=self.params[\"learning_rate\"])\n",
    "        opt   = tfa.optimizers.SWA(opt)\n",
    "        model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])\n",
    "        return model        \n",
    "    \n",
    "    # function that decrease the learning as epochs increase (i also change this part of the code)\n",
    "    def lr_schedule(self, epoch):\n",
    "        if   epoch < 30: return self.params[\"learning_rate\"]\n",
    "        elif epoch < 40: return self.params[\"learning_rate\"] / 3\n",
    "        elif epoch < 50: return self.params[\"learning_rate\"] / 5\n",
    "        elif epoch < 60: return self.params[\"learning_rate\"] / 7\n",
    "        elif epoch < 70: return self.params[\"learning_rate\"] / 9\n",
    "        elif epoch < 80: return self.params[\"learning_rate\"] / 11\n",
    "        elif epoch < 90: return self.params[\"learning_rate\"] / 13\n",
    "        else:            return self.params[\"learning_rate\"] / 100\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        # Prepare WaveNet model\n",
    "        K.clear_session()\n",
    "        config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "        sess   = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "        model  = self.get_model((None, len(self.features)))\n",
    "        # Define callbacks\n",
    "        cb_es  = EarlyStopping(monitor='val_loss', patience=self.params[\"patience\"], verbose=1, mode='auto')\n",
    "        cb_lr  = LearningRateScheduler(self.lr_schedule)\n",
    "        # Start training\n",
    "        model.fit(train_set[0], train_set[1],\n",
    "                  epochs          = self.params[\"nn_epochs\"],\n",
    "                  batch_size      = self.params[\"nn_batch_size\"],\n",
    "                  validation_data = (val_set[0], val_set[1]),\n",
    "                  callbacks       = [cb_lr, cb_es],\n",
    "                  verbose         = 2)\n",
    "        return model, None\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train=None, x_val=None, y_val=None):\n",
    "        x_ary     = np.array(x_train)\n",
    "        train_set = [x_ary.reshape (-1, self.params[\"batch_size\"], len(self.features))]\n",
    "        if y_train is None:\n",
    "            return train_set\n",
    "        yt_ohe = to_categorical(y_train, num_classes=self.params[\"num_classes\"])\n",
    "        train_set.append(yt_ohe.reshape(-1, self.params[\"batch_size\"], self.params[\"num_classes\"]))\n",
    "        \n",
    "        if x_val is not None:\n",
    "            x_ary   = np.array(x_val)\n",
    "            yv_ohe  = to_categorical(y_val, num_classes=self.params[\"num_classes\"])\n",
    "            val_set = [x_ary.reshape (-1, self.params[\"batch_size\"], len(self.features)),\n",
    "                       yv_ohe.reshape(-1, self.params[\"batch_size\"], self.params[\"num_classes\"])]\n",
    "            return train_set, val_set\n",
    "        return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_corr(df, thrs=[0.99]):\n",
    "    dict_features = {}\n",
    "    for thr in thrs:\n",
    "        dict_features[str(thr)] = []\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            for thr in thrs:\n",
    "                if thr < abs(corr_matrix.iloc[i, j]):\n",
    "                    dict_features[str(thr)].append(corr_matrix.columns[i])\n",
    "    for key, item in dict_features.items():\n",
    "        dict_features[key] = sorted(set(dict_features[key]))\n",
    "        print(key, \"The number of features is %s\" % len(dict_features[key]))\n",
    "    del corr_matrix\n",
    "    return dict_features\n",
    "\n",
    "def plot_feature_importance(df, n_splits=1, rank_range=[1, 50]):\n",
    "    fig, ax   = plt.subplots(1, 1, figsize=(10, 20))\n",
    "    sorted_df = df.sort_values(by=\"importance_mean\", ascending=False).reset_index()\n",
    "    sns.barplot(data=sorted_df.iloc[n_splits*(rank_range[0]-1) : n_splits*rank_range[1]],\n",
    "                x=\"importance\", y=\"features\", orient='h')\n",
    "    ax.set_xlabel(\"feature importance\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    return sorted_df\n",
    "\n",
    "def plot_results(df, col, thin_out=10):\n",
    "    for v in sorted(df[col].unique()):\n",
    "        df_group = df[df[col]==v]\n",
    "        x = df_group.time[::thin_out];\n",
    "        y = df_group.signal.values[::thin_out]\n",
    "        plt.scatter(x, y)\n",
    "    plt.legend(sorted(df[col].unique()), bbox_to_anchor=(1.05, 1), loc='best', borderaxespad=0)\n",
    "    \n",
    "def concat_no_imp_feats(df, no_feats, thr_no_imp):\n",
    "    df = df.copy()\n",
    "    df[\"use\"] = True\n",
    "    df_no_imp = pd.read_csv(\"../input/results-for-ionswitching/features_of_lgbm_regression.csv\")\n",
    "    df_no_imp = df_no_imp.query(\"features in @no_feats\")[[\"features\",\"importance_mean\"]].drop_duplicates()\n",
    "    df_no_imp[\"use\"] = False\n",
    "    return pd.concat([df, df_no_imp], sort=False).sort_values(by=\"importance_mean\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create WaveNet and LGBM, and ensemble with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate correlation of features, and the features that have high correlation will be deleted\n",
    "drop_cols     = [\"time\",\"open_channels\",\"batch\",\"batch2\"]\n",
    "df_for_corr   = train.sample(int(train.shape[0]/10), random_state=42)\n",
    "df_for_corr   = df_for_corr.reset_index(drop=True).drop(drop_cols, axis=1).copy()\n",
    "dict_features = feature_extraction_corr(df_for_corr, thrs=[0.95, 0.96, 0.97, 0.98, 0.99])\n",
    "\n",
    "feat_corr_95 = dict_features[\"0.95\"]; feat_corr_96 = dict_features[\"0.96\"]\n",
    "feat_corr_97 = dict_features[\"0.97\"]; feat_corr_98 = dict_features[\"0.98\"]\n",
    "feat_corr_99 = dict_features[\"0.99\"]\n",
    "del df_for_corr, dict_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cols(feat_corr_95, 200)\n",
    "show_cols(feat_corr_99, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of training\n",
    "target    = \"open_channels\"\n",
    "fold      = \"GroupKFold\"\n",
    "group     = \"batch2\"\n",
    "task_wn   = \"nn_multiclass\"\n",
    "task_lgbm = \"regression\" if LGBM_REGRESSION else \"multiclass\"\n",
    "\n",
    "if TRAIN_WAVENET:\n",
    "    # WaveNet paramters\n",
    "    k = 6; thr_tr_subs_ratio = 1\n",
    "elif task_lgbm == \"regression\":\n",
    "    # Regression task on LGBM parameters\n",
    "    k = 6; thr_tr_subs_ratio = 5\n",
    "else:\n",
    "    # Multiclass task on LGBM  parameters\n",
    "    k = 4; thr_tr_subs_ratio = 10\n",
    "    \n",
    "# Threshold of subsample for validation\n",
    "thr_va_subs_num = 2\n",
    "\n",
    "# We have to do three trial on LGBM regression.\n",
    "#  The first trial is to decide hyperparameters, so set False.\n",
    "#  The second trial is to decide importance features by using that, so set False.\n",
    "#  The third trial is to create results for all data. That'll be used on LGBM multiclass and WaveNet, so set True.\n",
    "del_valid_data = False  # If true, validation data will be overridden by None\n",
    "\n",
    "# Training hyperparameters\n",
    "tune = False\n",
    "if tune:\n",
    "    params = None\n",
    "else:\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective' : task_lgbm,\n",
    "        'num_leaves': 889,\n",
    "        'max_depth' : 10,\n",
    "        'min_data_in_leaf': 50,\n",
    "        'min_child_weight': 7,\n",
    "        'feature_fraction': 0.441848950364408,\n",
    "        'bagging_fraction': 0.7907047350361494,\n",
    "        'bagging_freq' : 4,\n",
    "        'min_child_samples': 97,\n",
    "        'lambda_l1'    : 0.034185189341285234,\n",
    "        'lambda_l2'    : 9.744523836901568,\n",
    "        'learning_rate': 0.001,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'bagging_seed': 11,\n",
    "        'random_state': 42,\n",
    "        'verbosity'   : -1}\n",
    "\n",
    "    if task_lgbm==\"regression\":\n",
    "        params['metric']    = 'rmse'\n",
    "    else:\n",
    "        params['metric']    = \"multi_logloss\"\n",
    "        params['num_class'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difine features will be used on training\n",
    "not_use_cols = [\"time\",\"open_channels\",\"batch\",\"batch2\"]\n",
    "drop_cols    = feat_corr_99\n",
    "drop_cols    = [col for col in train.columns if col in drop_cols]\n",
    "features     = [col for col in train.columns if col not in not_use_cols + drop_cols]\n",
    "\n",
    "train.drop(drop_cols, axis=1, inplace=True)\n",
    "test .drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "print(\"The number of training features is %s,\" % len(features))\n",
    "show_cols(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data and validation dara\n",
    "train_data = train.query(\"batch2 not in (910,955)\").reset_index(drop=True)\n",
    "valid_data = pd.DataFrame()\n",
    "\n",
    "# Use some train data for validation\n",
    "if 0 < thr_va_subs_num:\n",
    "    for batch in train_data.batch.unique():\n",
    "        data        = train_data.query(\"batch==@batch\")\n",
    "        valid_batch = list(data.batch2.sample(thr_va_subs_num, random_state=42))\n",
    "        print(\"Batch is %s, and batch2 are got from range of %s to %s. Results of sampling are %s.\"\n",
    "              % (batch, data.batch2.min(), data.batch2.max(), valid_batch))\n",
    "        for vb in valid_batch:\n",
    "            valid_data = valid_data.append(data.query(\"batch2==@vb\"))\n",
    "\n",
    "    valid_data = valid_data.reset_index(drop=True)\n",
    "    train_data = train_data.query(\"batch2 not in @valid_data.batch2.unique()\").reset_index(drop=True)\n",
    "    del data\n",
    "    \n",
    "# If 'del_valid_data' is true, validation data will be overridden.\n",
    "if del_valid_data:\n",
    "    print(\"validation data were overridden by empty dataframe.\")\n",
    "    valid_data = pd.DataFrame()\n",
    "    \n",
    "# If 'thr_tr_subs_ratio' is over 1, training data will be subsampled.\n",
    "if 1 < thr_tr_subs_ratio and TRAIN_LGBM:\n",
    "    extract_num = int(train_data.shape[0]/thr_tr_subs_ratio)\n",
    "    train_data  = train_data.sample(extract_num, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "train_data.shape, valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if TRAIN_WAVENET:\n",
    "    seed_everything()\n",
    "    # Start training\n",
    "    wn = WaveNet(train_data, test, target, features,\n",
    "                 valid_df=valid_data, task=task_wn,\n",
    "                 cv_method=fold, n_splits=k, group=group,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if TRAIN_LGBM:\n",
    "    # Start training\n",
    "    lgbm = LgbModel(train_data, test, target, features,\n",
    "                    valid_df=valid_data, task=task_lgbm,\n",
    "                    cv_method=fold, n_splits=k, group=group, \n",
    "                    params=params, parameter_tuning=tune,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results for submission and next kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save predict results.\n",
    "train_preds_wn   = None; test_preds_wn   = None\n",
    "train_preds_lgbm = None; test_preds_lgbm = None\n",
    "\n",
    "if TRAIN_WAVENET:\n",
    "    print(\"Saving WaveNet results...\")\n",
    "    train_preds_wn = pd.DataFrame(wn.y_valid)\n",
    "    test_preds_wn  = pd.DataFrame(wn.y_pred)\n",
    "    train_preds_wn.to_csv(\"./wavenet_train_preds.csv\", index=False)\n",
    "    test_preds_wn .to_csv(\"./wavenet_test_preds.csv\",  index=False)\n",
    "\n",
    "if TRAIN_LGBM:\n",
    "    print(\"Saving LGBM results...\")\n",
    "    if lgbm.y_valid is None:\n",
    "        train_preds_lgbm = pd.DataFrame(lgbm.oof)        \n",
    "    else:\n",
    "        train_preds_lgbm = pd.DataFrame(lgbm.y_valid)\n",
    "    test_preds_lgbm      = pd.DataFrame(lgbm.y_pred)\n",
    "    train_preds_lgbm.to_csv(\"./lgbm_%s_train_preds.csv\" % task_lgbm, index=False)        \n",
    "    test_preds_lgbm .to_csv(\"./lgbm_%s_test_preds.csv\"  % task_lgbm, index=False)\n",
    "    \n",
    "if ENSEMBLE_MODELS:\n",
    "    # If model training has not been run, the previous kernel results are loaded.\n",
    "    if train_preds_wn is None:\n",
    "        train_preds_wn   = pd.read_csv(\"../input/results-for-ionswitching/wavenet_train_preds.csv\")\n",
    "        test_preds_wn    = pd.read_csv(\"../input/results-for-ionswitching/wavenet_test_preds.csv\")\n",
    "        # Save for next kernels.\n",
    "        train_preds_wn.to_csv(\"./wavenet_train_preds.csv\", index=False)        \n",
    "        test_preds_wn .to_csv(\"./wavenet_test_preds.csv\",  index=False)\n",
    "    if train_preds_lgbm is None:\n",
    "        train_preds_lgbm = pd.read_csv(\"../input/results-for-ionswitching/lgbm_multiclass_train_preds.csv\")\n",
    "        test_preds_lgbm  = pd.read_csv(\"../input/results-for-ionswitching/lgbm_multiclass_test_preds.csv\")\n",
    "        # Save for next kernels.\n",
    "        train_preds_lgbm.to_csv(\"./lgbm_multiclass_train_preds.csv\", index=False)        \n",
    "        test_preds_lgbm .to_csv(\"./lgbm_multiclass_test_preds.csv\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show scores\n",
    "if TRAIN_WAVENET:\n",
    "    train_score = wn.score\n",
    "    del wn\n",
    "if TRAIN_LGBM:\n",
    "    train_score = lgbm.score\n",
    "    fi_df       = lgbm.fi_df.copy()\n",
    "    display(lgbm.params)    \n",
    "    del lgbm\n",
    "\n",
    "if 0 < valid_data.shape[0]:\n",
    "    gt = valid_data.open_channels\n",
    "    if TRAIN_WAVENET:\n",
    "        valid_preds = np.argmax(np.array(train_preds_wn), axis=1)\n",
    "    if TRAIN_LGBM:\n",
    "        valid_preds = np.argmax(np.array(train_preds_lgbm), axis=1)\n",
    "    valid_score = f1_score(gt, valid_preds, average='macro')\n",
    "else:\n",
    "    valid_score = None\n",
    "\n",
    "print(\"Training  : %s\" % train_score)\n",
    "print(\"validation: %s\" % valid_score)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Search the best parameters for ensembling.\n",
    "if ENSEMBLE_MODELS:\n",
    "    gt = valid_data.open_channels\n",
    "    results = []\n",
    "    for a in np.arange(0, 1.001, 0.01):\n",
    "        preds = np.argmax(np.array(a*train_preds_wn) + np.array((1-a)*train_preds_lgbm), axis=1)\n",
    "        results.append([a, 1-a, f1_score(gt, preds, average='macro')])\n",
    "    df_ensembled = pd.DataFrame(results, columns=[\"WaveNet_ratio\",\"LGBM_ratio\",\"score\"])\n",
    "    wn_a, lgbm_a ,best_score = df_ensembled.query(\"score==@df_ensembled.score.max()\").values[0]\n",
    "    \n",
    "    print(\"Best parameters are %s(WaveNet) and %s(LGBM). Best score is %s.\" % (wn_a, lgbm_a, best_score))\n",
    "    print(\"==========================\")\n",
    "    df_ensembled.plot(x=\"WaveNet_ratio\",y=\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission files\n",
    "if ENSEMBLE_MODELS:\n",
    "    print(\"Use %s(WaveNet) and %s(LGBM).\" % (wn_a, lgbm_a))\n",
    "    test.open_channels = np.argmax(np.array(wn_a*test_preds_wn) + np.array(lgbm_a*test_preds_lgbm), axis=1).astype(int)\n",
    "    test[[\"time\",\"open_channels\"]].to_csv('./submission_ensembled.csv', index=False, float_format='%0.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of open channels for training\n",
    "if ENSEMBLE_MODELS:\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plot_results(train_data, \"open_channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of open channels for testing\n",
    "if ENSEMBLE_MODELS:\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plot_results(test, \"open_channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission files\n",
    "if TRAIN_WAVENET:\n",
    "    test.open_channels = np.argmax(np.array(test_preds_wn), axis=1).astype(int)\n",
    "    test[[\"time\",\"open_channels\"]].to_csv('./submission_wavenet.csv', index=False, float_format='%0.4f')\n",
    "    \n",
    "if TRAIN_LGBM:\n",
    "    if task_lgbm == \"multiclass\":\n",
    "        test.open_channels = np.argmax(np.array(test_preds_lgbm), axis=1).astype(int)\n",
    "    else:\n",
    "        test.open_channels = np.round(np.clip(np.array(test_preds_lgbm), 0, 10)).astype(int)\n",
    "    test[[\"time\",\"open_channels\"]].to_csv('./submission_lgbm_%s.csv' % task_lgbm, index=False, float_format='%0.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of open channels for testing\n",
    "if TRAIN_WAVENET or TRAIN_LGBM:\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plot_results(test, \"open_channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "if TRAIN_LGBM:\n",
    "    df_feat_sorted = plot_feature_importance(fi_df, n_splits=k)\n",
    "    df_feat_sorted = concat_no_imp_feats(fi_df, no_feats, thr_no_imp)\n",
    "    if OUTPUT_FEATURES:\n",
    "        df_feat_sorted.to_csv(\"./features_of_lgbm_%s.csv\" % task_lgbm, index=False)\n",
    "    display(df_feat_sorted[[\"features\",\"importance_mean\",\"use\"]].drop_duplicates().head(20))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
